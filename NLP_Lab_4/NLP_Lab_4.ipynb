{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48174d40-c73f-4f52-96e6-aa7add6ddc6b",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center\">NLP Lab 4</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e33619-464c-42cd-8f67-9fcf2a57acc5",
   "metadata": {},
   "source": [
    "# Performed by : **Idriss Khattabi**\n",
    "# Supervised by: **AACHAK Lotfi**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbcfb62-760d-44f4-8ba9-95fa334a7f22",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "---------------\n",
    "### **Objective** : The main purpose behind this lab is to get familiar with NLP language models using Pytorch library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd7f6ab-0cc6-4655-9e05-1c693d446117",
   "metadata": {},
   "source": [
    "## part 1 :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6849ac06-a3e9-4790-94bb-95dbb34a9400",
   "metadata": {},
   "source": [
    "#### 1- By using scrapping libraries (Scrapy / BeautifulSoup), try to collect text data from several Arabic web site concerning one topic then prepare your Dataset The score presents the relevance of each text (The score should be between 0 to 10)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082c37e6-0e7d-40c2-8056-d1b47adb28a3",
   "metadata": {},
   "source": [
    "#### => lets scrap data about artificial intelligence from arabic websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "70bb09d1-9929-48cf-94d9-a20059c3cf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for Data Scraping\n",
    "import requests, re, csv\n",
    "from bs4 import BeautifulSoup, NavigableString\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "86b5207d-34b6-4096-b5b5-d697973b3ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove tags with its content except the specified ones\n",
    "def remove_tags(soup, preserve_tags):\n",
    "    for tag in soup.find_all(True):  # Find all tags\n",
    "        if tag.name not in preserve_tags:\n",
    "            tag.decompose()  # Remove the tag with its content\n",
    "\n",
    "def text_cleaning(text) :\n",
    "    # Remove HTML tags \n",
    "    text = re.sub('<[^<]+?>', ' ', text)\n",
    "    # Remove non-Arabic characters, digits, and punctuation\n",
    "    text = re.sub(r'[^\\u0600-\\u06FF\\s]', ' ', text)\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "e21ce47c-8864-4472-ab6a-956049fe110d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrip(url, special_tag, preserve_tags, will_remove_tags=True, headers=None) :\n",
    "    result = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(result.content, \"lxml\")\n",
    "    if special_tag[1] != None :\n",
    "        articles_container = soup.find(special_tag[0], special_tag[1])\n",
    "    else : \n",
    "        articles_container = soup.find(special_tag[0])\n",
    "\n",
    "    if will_remove_tags :\n",
    "        remove_tags(articles_container, preserve_tags)\n",
    "    \n",
    "    cleaned_content = text_cleaning(str(articles_container))\n",
    "    \n",
    "    return cleaned_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "421e0fde-0c25-4709-ae91-f8f20515cc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://mawdoo3.com/%D8%AA%D8%B9%D8%B1%D9%8A%D9%81_%D8%A7%D9%84%D8%B0%D9%83%D8%A7%D8%A1_%D8%A7%D9%84%D8%A7%D8%B5%D8%B7%D9%86%D8%A7%D8%B9%D9%8A\",\n",
    "    \"https://ar.wikipedia.org/wiki/%D8%B0%D9%83%D8%A7%D8%A1_%D8%A7%D8%B5%D8%B7%D9%86%D8%A7%D8%B9%D9%8A\",\n",
    "    \"https://www.aljazeera.net/tech/2016/5/4/%D8%A7%D9%84%D8%B0%D9%83%D8%A7%D8%A1-%D8%A7%D9%84%D8%A7%D8%B5%D8%B7%D9%86%D8%A7%D8%B9%D9%8A-%D9%85%D8%A7-%D9%87%D9%88-%D9%88%D9%85%D8%A7-%D8%A3%D8%A8%D8%B1%D8%B2-%D9%85%D8%B8%D8%A7%D9%87%D8%B1%D9%87\",\n",
    "    \"https://aws.amazon.com/ar/what-is/artificial-intelligence/\",\n",
    "    \"https://www.oracle.com/ae-ar/artificial-intelligence/what-is-ai/\",\n",
    "    \"https://www.sap.com/mena-ar/products/artificial-intelligence/what-is-artificial-intelligence.html\",\n",
    "    \"https://attaa.sa/library/view/1279\",\n",
    "    \"https://www.jamous-tech.com/artificial-intelligence-ai/\",\n",
    "    \"https://www.hindawi.org/books/48149074/1/\",\n",
    "    \"https://www.annajah.net/%D8%A7%D9%84%D8%B0%D9%83%D8%A7%D8%A1-%D8%A7%D9%84%D8%A7%D8%B5%D8%B7%D9%86%D8%A7%D8%B9%D9%8A-%D8%AA%D8%B9%D8%B1%D9%8A%D9%81%D9%87-%D9%88%D8%A3%D9%87%D9%85%D9%8A%D8%AA%D9%87-%D9%88%D8%A3%D9%86%D9%88%D8%A7%D8%B9%D9%87-%D9%88%D8%A3%D9%87%D9%85-%D8%AA%D8%B7%D8%A8%D9%8A%D9%82%D8%A7%D8%AA%D9%87-article-30227\",\n",
    "    \"https://sdaia.gov.sa/ar/SDAIA/about/Pages/AboutAI.aspx\",\n",
    "    \"https://motaber.com/artificial-intelligence/\",\n",
    "    \"https://www.argaam.com/ar/article/articledetail/id/1615530\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ce347ea3-6bdc-4678-a036-be8255e912dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_articles = []\n",
    "for url, i in zip(urls, range(len(urls))) :\n",
    "    will_remove_tags = True\n",
    "    if i == 7 :\n",
    "        will_remove_tags = False\n",
    "\n",
    "    headers = None\n",
    "    if i == 5 :\n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\",\n",
    "            \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "            \"Accept-Language\": \"ar\",\n",
    "            \"Connection\": \"keep-alive\",\n",
    "        }\n",
    "        \n",
    "    preserve_tags = {'p', 'h2', 'h3', 'div', 'b'}\n",
    "    if i == 4 : \n",
    "        preserve_tags = {'p', 'h2', 'h3', 'div', 'section'}\n",
    "    if i == 5 or i == 6: \n",
    "        preserve_tags = {'p', 'h2', 'h3', 'div', 'section', 'span'}\n",
    "    if i == 8 or i == 9 or i == 10: \n",
    "        preserve_tags = {'p', 'h2', 'h3', 'h5', 'div', 'ul', 'li' 'section', 'span'}\n",
    "    if i == 11 or i == 12: \n",
    "        preserve_tags = {'p', 'h2', 'h3', 'h5', 'div', 'section', 'span', 'table', 'tbody', 'tr', 'td'}\n",
    "\n",
    "    special_tag = (\"div\", {\"id\": \"mw-content-text\"})\n",
    "    if i == 2 : special_tag = (\"main\", {\"id\": \"main-content-area\"})\n",
    "    if i == 3 : special_tag = (\"div\", {\"class\": \"eb-faq-content\"})\n",
    "    if i == 4 : special_tag = (\"div\", {\"class\": \"f22w1\"})\n",
    "    if i == 5 : special_tag = (\"div\", {\"class\": \"parContent\"})\n",
    "    if i == 6 : special_tag = (\"div\", {\"class\": \"paragraph-display\"})\n",
    "    if i == 7 : special_tag = (\"div\", {\"class\": \"entry-content single-content\"})\n",
    "    if i == 8 : special_tag = (\"article\", None)\n",
    "    if i == 9 : special_tag = (\"article\", {'id': 'article-details'})\n",
    "    if i == 10 : special_tag = (\"div\", {\"class\": \"article-content\"})\n",
    "    if i == 11 : special_tag = (\"div\", {\"class\": \"single-content\"})\n",
    "    if i == 12 : special_tag = (\"div\", {\"id\": \"articledetail\"})\n",
    "\n",
    "    all_articles.append(scrip(url, special_tag, preserve_tags, will_remove_tags=will_remove_tags, headers=headers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "895ba58e-9844-437e-8668-fff12b174db8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' أرقام شارك انسَخ رابِط المَقالْ يُقصد بالذكاء الاصطناعي أيّ قدرة الآلة على تعلّم كيفية إكمال المهام دون تعليمات بشرية صريحة ويرجع الفضل في تطوير فكرة الذكاء الاصطناعي عمومًا لعالم الرياضيات البريطاني آلان تورينج، والذي كان يؤمن بأهمية وجود آلات التفكير لحل المشكلات بشكل مستقل تمامًا مثل البشر، وهو معيار يُعرف باسم اختبار تورنغ يقدم هذا التقرير نظرة عامة على الذكاء الاصطناعي، وتعريفه، وأمثلة على استخدام الذكاء الاصطناعي في التمويل وتدبير الموارد المالية لا يوجد تعريف موحد للذكاء الاصطناعي، ولكن هناك تعريفًا مقبولًا بشكل عام يصفه بأنه تلك الآلات التي تستجيب للمحفزات بشكل متسق مع الاستجابات التقليدية للبشر، أي بالمقارنة مع قدرة الإنسان على التأمل وإصدار الأحكام وتكوين الرأي الذكاء الاصطناعي هو قدرة عامة على استخدام معطيات الوقت الفعلي لاتخاذ القرار حيث يتلقى الجهاز أو البرنامج تلك المعطيات من خلال أجهزة الاستشعار أو الإدخال عن بُعد أو رقميًا، ثم يقوم بتحليلها قبل اتخاذ القرار، وهي السمة التي تميزها عن آلة مبرمجة مسبقًا في مجال التمويل، يمكن استخدام الذكاء الاصطناعي في عملية الاكتتاب لمساعدة المُقرض على اتخاذ قرارات أفضل فيما يتعلق بطلبات القروض بدلاً من الاعتماد على التحليلات التنبؤية التي يحددها الإحصائيون، يمكن لخوارزمية الكمبيوتر قراءة البيانات الخاصة بالقروض السابقة وتحديد أفضل نموذج تنبؤي لتقييم الجدارة الائتمانية لمقدمي الطلبات كذلك، يعتبر المستشار الآلي استخدامًا شائعًا آخر للذكاء الاصطناعي في مجال التمويل يستخدم المستشار الآلي معلومات العميل حول الأهداف المالية، وتحمل المخاطر، وأفق الاستثمار لتحديد تخصيص الأصول الاستثمارية ثم يقوم بعد ذلك بإعادة توازن المحفظة حسب الحاجة، ووضع الصفقات وحتى التعامل مع مهام مثل حصيلة الضرائب بشكل عام، هناك أربع فئات من الذكاء الاصطناعي وهي الآلات التفاعلية ، والذاكرة المحدودة ، ونظرية العقل ، والوعي الذاتي فكر في هذه الأنواع على أنها طيف تدريجي؛ كل نوع يعتمد على مدى تعقيد النوع الذي يسبقه هذا هو النوع الأساسي من الذكاء الاصطناعي يمكن للذكاء الاصطناعي التفاعلي أن يعمل بناءً على تقييم الوضع الحالي ولكنه غير قادر على بناء مستودع للذكريات للاستفادة منها في المستقبل يمكن للذكاء الاصطناعي ذي الذاكرة المحدودة تذكر التجارب السابقة على أنها تمثيلات مبرمجة مسبقًا لبيئتها ثم يقوم الذكاء الاصطناعي ذا الذاكرة المحدودة بدمج هذه الذكريات في القرارات المستقبلية هذا النوع من الذكاء الاصطناعي أكثر تقدمًا من الذاكرة المحدودة واسمها مأخوذ من المصطلح النفسي، يمكن للذكاء الاصطناعي لنظرية العقل أن ينسب الحالات العقلية مثل المعتقدات والنوايا والرغبة والعواطف والمعرفة للآخرين يتجاوز الذكاء الاصطناعي الواعي ذاتيًا الذكاء الاصطناعي القائم على نظرية العقل، فهو لديه القدرة على تكوين تمثيلات عن نفسه وبالتالي امتلاك الوعي وهنا يجدر الإشارة إلى أننا تجاوزنا اليوم مرحلة النوع الأول من الذكاء الاصطناعي، ونحن على وشك إتقان واحتراف النوع الثاني، إلا أن النوعيّن الثالث والرابع من الذكاء الاصطناعي يتواجدان كنظرية فقط، وسيمثّلان على الأغلب المرحلة المقبلة من تطوّر الذكاء الاصطناعي '"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_articles[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba36cdb-a5b1-4b2f-9015-2d839cfec4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = []\n",
    "# the scoring is depending of the length of the text\n",
    "scores = [7, 9, 5.5, 8, 7, 7.5, 6, 5, 8.5, 6.5, 6, 7.5, 5.5]\n",
    "\n",
    "for article, score in zip(all_articles, scores) :\n",
    "    articles.append({\n",
    "        \"Text\": article,\n",
    "        \"Score\": score,\n",
    "        \"WordNbr\" : len(article.split())\n",
    "    })\n",
    "\n",
    "data = pd.DataFrame(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "6db79e04-0cda-477e-af1e-b2436b8206a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************************** \n",
      " file created \n",
      " **************************************************\n"
     ]
    }
   ],
   "source": [
    "# Storing the scraped data in a CSV file\n",
    "columns = articles[0].keys()\n",
    "\n",
    "with open(\"arabic_texts.csv\", \"w\", encoding=\"utf-8-sig\", newline=\"\") as csv_file:\n",
    "    dict_writer = csv.DictWriter(csv_file, columns)\n",
    "    dict_writer.writeheader()\n",
    "    dict_writer.writerows(articles)\n",
    "\n",
    "    print(\"*\"*50, \"\\n file created \\n\", \"*\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ff5bd3d6-44f8-421e-ad49-12f8ab6bd2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"arabic_texts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "3cc8bd5e-bdd6-437e-acdf-aec2fd5ced9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Score</th>\n",
       "      <th>WordNbr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>محتويات يرتبط مفهوم الذكاء الاصطناعي بالإنجلي...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ذكاء صناعي الْذَكَاءُ الْاِصْطِنَاعِيُّ أو ال...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>بات مصطلح الذكاء الاصطناعي كثير الاستخدام هذه...</td>\n",
       "      <td>5.5</td>\n",
       "      <td>664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ما المقصود بالذكاء الاصطناعي؟ الذكاء الاصطناع...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>استمتع بإنشاء التطبيقات واختبارها ونشرها على ...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>جاء تعريف مبكر للذكاء الاصطناعي من أحد آبائها...</td>\n",
       "      <td>7.5</td>\n",
       "      <td>1501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>يعرف الذكاء الاصطناعي الذي يسمى اختصاراً ويعد...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>في ظل التطورات السريعة في الإنترنت ربما تكون ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>الفصل الأول الهدف الأساسي من الذكاء الاصطناعي...</td>\n",
       "      <td>8.5</td>\n",
       "      <td>4523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>تعريف الذكاء الاصطناعي يعد فرعاً من علوم الكم...</td>\n",
       "      <td>6.5</td>\n",
       "      <td>723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>الذكاء الاصطناعي نبذة عن الذكاء الاصطناعي على...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>يعد حاليًا أحد أهم الكلمات الطنانة في مجال ال...</td>\n",
       "      <td>7.5</td>\n",
       "      <td>3426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>أرقام شارك انسَخ رابِط المَقالْ يُقصد بالذكاء...</td>\n",
       "      <td>5.5</td>\n",
       "      <td>430</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Text  Score  WordNbr\n",
       "0    محتويات يرتبط مفهوم الذكاء الاصطناعي بالإنجلي...    7.0     1031\n",
       "1    ذكاء صناعي الْذَكَاءُ الْاِصْطِنَاعِيُّ أو ال...    9.0     3833\n",
       "2    بات مصطلح الذكاء الاصطناعي كثير الاستخدام هذه...    5.5      664\n",
       "3    ما المقصود بالذكاء الاصطناعي؟ الذكاء الاصطناع...    8.0     1984\n",
       "4    استمتع بإنشاء التطبيقات واختبارها ونشرها على ...    7.0     1215\n",
       "5    جاء تعريف مبكر للذكاء الاصطناعي من أحد آبائها...    7.5     1501\n",
       "6    يعرف الذكاء الاصطناعي الذي يسمى اختصاراً ويعد...    6.0     1092\n",
       "7    في ظل التطورات السريعة في الإنترنت ربما تكون ...    5.0     1254\n",
       "8    الفصل الأول الهدف الأساسي من الذكاء الاصطناعي...    8.5     4523\n",
       "9    تعريف الذكاء الاصطناعي يعد فرعاً من علوم الكم...    6.5      723\n",
       "10   الذكاء الاصطناعي نبذة عن الذكاء الاصطناعي على...    6.0      698\n",
       "11   يعد حاليًا أحد أهم الكلمات الطنانة في مجال ال...    7.5     3426\n",
       "12   أرقام شارك انسَخ رابِط المَقالْ يُقصد بالذكاء...    5.5      430"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd99e6b-f152-43d1-bf57-13d3fa97b91f",
   "metadata": {},
   "source": [
    "#### 2. Establish a preprocessing NLP pipeline (Tokenization stemming lemmatization, Stop words, Discretization, etc) of the collected Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "7731512c-8092-4c6f-8a21-8b10a09b44dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from camel_tools.utils.dediac import dediac_ar\n",
    "from camel_tools.utils.normalize import normalize_alef_maksura_ar, normalize_teh_marbuta_ar\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from farasa.stemmer import FarasaStemmer\n",
    "\n",
    "# Download NLTK stopwords\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "\n",
    "# Initialize components\n",
    "stop_words = set(stopwords.words('arabic'))\n",
    "stemmer = FarasaStemmer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Normalize text\n",
    "    text = dediac_ar(text)  # Remove diacritics\n",
    "    text = normalize_alef_maksura_ar(text)  # Normalize Alef Maksura\n",
    "    text = normalize_teh_marbuta_ar(text)  # Normalize Teh Marbuta\n",
    "    \n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stop words\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Removing punctuation\n",
    "    tokens = [token for token in tokens if token not in string.punctuation]\n",
    "    \n",
    "    # Remove '،' from each token\n",
    "    tokens_lemma = [token.replace('،', '') for token in tokens]\n",
    "    \n",
    "    # Filter out empty tokens\n",
    "    tokens_lemma = [token for token in tokens_lemma if token]\n",
    "    \n",
    "    # FarasaStemmer\n",
    "    stemmed = stemmer.stem(\" \".join(tokens_lemma))\n",
    "    \n",
    "    return stemmed, len(tokens_lemma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "0145f533-8612-46b2-993c-89a22ee509dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_texts = []\n",
    "nbr_words_processed_texts = []\n",
    "\n",
    "for text in data['Text'] :\n",
    "    preprocess_text1 , len_text = preprocess_text(text)\n",
    "    processed_texts.append(preprocess_text1)\n",
    "    nbr_words_processed_texts.append(len_text)\n",
    "    \n",
    "data['processed_text'] = processed_texts\n",
    "data['ProcessedWordNbr'] = nbr_words_processed_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "d5939519-8786-4194-b1ba-97c8d37ab93b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Score</th>\n",
       "      <th>WordNbr</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>ProcessedWordNbr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>محتويات يرتبط مفهوم الذكاء الاصطناعي بالإنجلي...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1031</td>\n",
       "      <td>محتوى ارتبط مفهوم ذكاء اصطناعي إنجليزي ذكاء مر...</td>\n",
       "      <td>860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ذكاء صناعي الْذَكَاءُ الْاِصْطِنَاعِيُّ أو ال...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3833</td>\n",
       "      <td>ذكاء صناعي ذكاء اصطناعي ذكاء صناعي ذكاء صنعي خ...</td>\n",
       "      <td>2964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>بات مصطلح الذكاء الاصطناعي كثير الاستخدام هذه...</td>\n",
       "      <td>5.5</td>\n",
       "      <td>664</td>\n",
       "      <td>مصطلح ذكاء اصطناعي كثير استخدام يوم درج بعض تخ...</td>\n",
       "      <td>535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ما المقصود بالذكاء الاصطناعي؟ الذكاء الاصطناع...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1984</td>\n",
       "      <td>مقصود ذكاء اصطناعي ؟ ذكاء اصطناعي مجال علم كمب...</td>\n",
       "      <td>1689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>استمتع بإنشاء التطبيقات واختبارها ونشرها على ...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1215</td>\n",
       "      <td>استمتع إنشاء تطبيق اختبار نشر علي مجان مصطلح ذ...</td>\n",
       "      <td>1001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>جاء تعريف مبكر للذكاء الاصطناعي من أحد آبائها...</td>\n",
       "      <td>7.5</td>\n",
       "      <td>1501</td>\n",
       "      <td>جاء تعريف مبكر ذكاء اصطناعي آباء مؤسس وصف أن ص...</td>\n",
       "      <td>1178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>يعرف الذكاء الاصطناعي الذي يسمى اختصاراً ويعد...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1092</td>\n",
       "      <td>عرف ذكاء اصطناعي سمى اختصار عد فرع فرع علم حاس...</td>\n",
       "      <td>875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>في ظل التطورات السريعة في الإنترنت ربما تكون ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1254</td>\n",
       "      <td>ظل تطور سريع إنترنت رب كان سمع ذكاء اصطناعي أخ...</td>\n",
       "      <td>1002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>الفصل الأول الهدف الأساسي من الذكاء الاصطناعي...</td>\n",
       "      <td>8.5</td>\n",
       "      <td>4523</td>\n",
       "      <td>فصل أول هدف أساسي ذكاء اصطناعي تمكين أجهز كمبي...</td>\n",
       "      <td>3552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>تعريف الذكاء الاصطناعي يعد فرعاً من علوم الكم...</td>\n",
       "      <td>6.5</td>\n",
       "      <td>723</td>\n",
       "      <td>تعريف ذكاء اصطناعي عد فرع علم كمبيوتر هدف إلى ...</td>\n",
       "      <td>610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>الذكاء الاصطناعي نبذة عن الذكاء الاصطناعي على...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>698</td>\n",
       "      <td>ذكاء اصطناعي نبذ ذكاء اصطناعي علي رغم ظهور مصط...</td>\n",
       "      <td>589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>يعد حاليًا أحد أهم الكلمات الطنانة في مجال ال...</td>\n",
       "      <td>7.5</td>\n",
       "      <td>3426</td>\n",
       "      <td>عد حالي أهم كلمة طنانه مجال تكنولوجيا سبب وجيه...</td>\n",
       "      <td>2649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>أرقام شارك انسَخ رابِط المَقالْ يُقصد بالذكاء...</td>\n",
       "      <td>5.5</td>\n",
       "      <td>430</td>\n",
       "      <td>رقم شارك انسخ رابط مقال قصد ذكاء اصطناعي قدر آ...</td>\n",
       "      <td>353</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Text  Score  WordNbr  \\\n",
       "0    محتويات يرتبط مفهوم الذكاء الاصطناعي بالإنجلي...    7.0     1031   \n",
       "1    ذكاء صناعي الْذَكَاءُ الْاِصْطِنَاعِيُّ أو ال...    9.0     3833   \n",
       "2    بات مصطلح الذكاء الاصطناعي كثير الاستخدام هذه...    5.5      664   \n",
       "3    ما المقصود بالذكاء الاصطناعي؟ الذكاء الاصطناع...    8.0     1984   \n",
       "4    استمتع بإنشاء التطبيقات واختبارها ونشرها على ...    7.0     1215   \n",
       "5    جاء تعريف مبكر للذكاء الاصطناعي من أحد آبائها...    7.5     1501   \n",
       "6    يعرف الذكاء الاصطناعي الذي يسمى اختصاراً ويعد...    6.0     1092   \n",
       "7    في ظل التطورات السريعة في الإنترنت ربما تكون ...    5.0     1254   \n",
       "8    الفصل الأول الهدف الأساسي من الذكاء الاصطناعي...    8.5     4523   \n",
       "9    تعريف الذكاء الاصطناعي يعد فرعاً من علوم الكم...    6.5      723   \n",
       "10   الذكاء الاصطناعي نبذة عن الذكاء الاصطناعي على...    6.0      698   \n",
       "11   يعد حاليًا أحد أهم الكلمات الطنانة في مجال ال...    7.5     3426   \n",
       "12   أرقام شارك انسَخ رابِط المَقالْ يُقصد بالذكاء...    5.5      430   \n",
       "\n",
       "                                       processed_text  ProcessedWordNbr  \n",
       "0   محتوى ارتبط مفهوم ذكاء اصطناعي إنجليزي ذكاء مر...               860  \n",
       "1   ذكاء صناعي ذكاء اصطناعي ذكاء صناعي ذكاء صنعي خ...              2964  \n",
       "2   مصطلح ذكاء اصطناعي كثير استخدام يوم درج بعض تخ...               535  \n",
       "3   مقصود ذكاء اصطناعي ؟ ذكاء اصطناعي مجال علم كمب...              1689  \n",
       "4   استمتع إنشاء تطبيق اختبار نشر علي مجان مصطلح ذ...              1001  \n",
       "5   جاء تعريف مبكر ذكاء اصطناعي آباء مؤسس وصف أن ص...              1178  \n",
       "6   عرف ذكاء اصطناعي سمى اختصار عد فرع فرع علم حاس...               875  \n",
       "7   ظل تطور سريع إنترنت رب كان سمع ذكاء اصطناعي أخ...              1002  \n",
       "8   فصل أول هدف أساسي ذكاء اصطناعي تمكين أجهز كمبي...              3552  \n",
       "9   تعريف ذكاء اصطناعي عد فرع علم كمبيوتر هدف إلى ...               610  \n",
       "10  ذكاء اصطناعي نبذ ذكاء اصطناعي علي رغم ظهور مصط...               589  \n",
       "11  عد حالي أهم كلمة طنانه مجال تكنولوجيا سبب وجيه...              2649  \n",
       "12  رقم شارك انسخ رابط مقال قصد ذكاء اصطناعي قدر آ...               353  "
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "27089102-c2a5-4190-8ba4-2386ddbb0b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text :\n",
      " بات مصطلح الذكاء الاصطناعي كثير الاستخدام هذه الأيام، لدرجة أن البعض أصبح يتخوف من أنه قد يعني سيطرة الآلات واضمحلال دور البشر، رغم أن الواقع ما يزال بعيدا جدا عن الاقتراب من هذا التصور، ف يعرف الذكاء الاصطناعي بأنه الذكاء الذي تبديه الآلات والبرامج بما يحاكي القدرات الذهنية البشرية وأنماط عملها، مثل القدرة على التعلم والاستنتاج ورد الفعل على أوضاع لم تبرمج في الآلة، كما أنه اسم لحقل أكاديمي يعنى بكيفية صنع حواسيب وبرامج قادرة على اتخاذ سلوك ذكي ويعرف كبار الباحثين الذكاء الاصطناعي بأنه دراسة وتصميم أنظمة ذكية تستوعب بيئتها وتتخذ إجراءات تزيد من فرص نجاحها ، في حين يعرفه جون مكارثي الذي وضع هذا المصطلح سنة بأنه علم وهندسة صنع آلات ذكية وخلال السنوات الأخيرة، قفز التطور في تقنية الذكاء الاصطناعي قفزات كبيرة، وتعد تقنية التعلم العميق أبرز مظاهره، وهي ترتكز على تطوير شبكات عصبية صناعية تحاكي في طريقة عملها أسلوب الدماغ البشري، أي أنها قادرة على التجريب والتعلم وتطوير نفسها ذاتيا دون تدخل الإنسان وأثبتت تقنية التعلم العميق قدرتها على التعرف على الصور وفهم الكلام والترجمة من لغة إلى أخرى، وغير ذلك من القدرات التي أغرت الشركات الأميركية في ، وتحديدا فيسبوك وغوغل، على الاستثمار وتكثيف الأبحاث فيها، متجاهلين تحذيرات من أن تطور الذكاء الاصطناعي قد يهدد البشرية ففي أواخر أشار عالم الفيزياء الراحل ستيفن هوكينغ إلى أن تطوير ذكاء اصطناعي كامل قد يمهد لفناء الجنس البشري، محذرا من قدرة الآلات على إعادة تصميم نفسها ذاتيا كما أعلن المؤسس والرئيس السابق لشركة مايكروسوفت بيل غيتس في عن رغبته في بقاء الروبوتات غبية إلى حد ما، وقال أنا في معسكر من يشعر بالقلق إزاء الذكاء الخارق وفي العام ذاته وصف المخترع والمستثمر الأميركي إيلون ماسك الذكاء الاصطناعي بأنه من أعظم المخاطر التي تهدد الوجود البشري، كما شبه تطوير الآلات الذكية باستحضار الشيطان ويستثمر ماسك مؤسس مشروع صواريخ الفضاء التجارية سبيس إكس، وسيارات تسلا الكهربائية وغيره ملايين الدولارات في أبحاث لاكتشاف المخاطر المحتملة للذكاء الاصطناعي وكيفية التعامل معها في المقابل، يرى بعض الخبراء أن تقنيات الذكاء الاصطناعي لن تتسبب في أي مخاطر على الجنس البشري، ومن هؤلاء أستاذ علم الحاسوب بجامعة مونتريال الكندي يوشوا بينغيو، الذي يرى أنه لا ينبغي القلق من التقنيات الذكية، فهي تحتاج لسنوات كثيرة من التطور البطيء والتدريجي قبل أن تصل إلى المدى الذي يخشاه المحللون، لأنها تستند في تطورها إلى علوم وأفكار ما تزال في بداياتها الأولى حاليا ويؤكد بينغيو أن الوصول إلى الذكاء الاصطناعي بشكله المنتظر لن يكون مفاجئا، أي ليس كما يشبهه البعض باكتشاف وصفة سحرية خارقة على حد تعبيره، فما زال إنتاج أنظمة الذكاء الاصطناعي المتكاملة بحاجة إلى تطور علوم حالية وابتكار علوم جديدة، أي بتعبير آخر لن يخرج أحد العلماء بتقنية ذكية من شأنها تغيير العالم بين ليلة وضحاها، كما في أفلام الخيال العلمي من ناحية أخرى، فإن تقنيات الذكاء الاصطناعي تتطور بسرعة كبيرة، وتصبح أكثر تعقيدا كل عام، ويرى الباحث المتخصص في مجال الذكاء الاصطناعي لدى شركة غوغل وجامعة تورنتو جيوفري هينتون أن الآلات ستوازي الإنسان ذكاءً خلال خمسة أعوام من الآن ويقف هينتون خلف تطوير برنامج غوغل الذكي ألفاغو الذي هزم بطل العالم في لعبة غو ، لكنه لا يرى أن علينا خشية الذكاء الاصطناعي، لأن أي تقنية جديدة قد تكون مثيرة للخوف في حال أسيء استخدامها، حسب قوله، وأن المسالة تتعلق بكيفية تعاملنا مع التكنولوجيا بشكل لا يجعل منها مؤذية للبشر وتعدّ شركتا غوغل وفيسبوك رائدتين في مجال تقنيات الذكاء الاصطناعي؛ فهذه التقنية تساعد غوغل على تطوير خدماتها بشكل كبير، فمثلا وبفضل هذه التقنية يمكن لهاتف أندرويد فهم أوامر مستخدمه، والترجمة الفورية للعبارات المكتوبة بلغة أجنبية على اللافتات في الطرقات، كما تسهم التقنية في دعم محرك البحث غوغل، الذي يعدّ أبرز منتجات الشركة أما بالنسبة لفيسبوك، فيسمح التعلم العميق للشبكة الاجتماعية بالتعرف على الوجوه في الصور، واختيار المحتوى المناسب وعرضه للمستخدم على صفحة آخر الأخبار، ودعم المساعد الشخصي الرقمي التابع لفيسبوك إم ، وغير ذلك من الوظائف ويرى المدير التنفيذي لشركة ألفابت الشركة الأم لغوغل سوندار بيشاي أن عصر الهواتف الذكية اقترب من نهايته ليُستبدل بالذكاء الاصطناعي الذي يتيح الوصول الفوري إلى المعلومات الضرورية، كما يرى مؤسس فيسبوك ورئيسها التنفيذي أن الأجهزة ذات الذكاء الاصطناعي ستستطيع يوما ما أن تتمتع بالحواس الإنسانية مثل الرؤية والشعور أكثر من البشر أنفسهم المصدر الجزيرة \n",
      "****************************************************************************************************\n",
      "processed_text : \n",
      "مصطلح ذكاء اصطناعي كثير استخدام يوم درج بعض تخوف أن عنى سيطر آلة اضمحلال دور بشر رغم واقع زال بعيد جد اقتراب تصور عرف ذكاء اصطناعي أن ذكاء أبدى آلة برنامج حاكى قدرة ذهنيه بشريه نمط عمل قدر علي تعلم استنتاج فعل علي وضع تبرمج آل أن اسم حقل أكاديمي عنى كيف صنع حاسوب برنامج قادر علي اتخاذ سلوك ذكي عرف كبير باحث ذكاء اصطناعي أن دراس تصميم نظم ذكي استوعب بيئة اتخذ إجراء زاد فرصة نجاح عرف جون مكارثي وضع مصطلح سن أن هندس صنع آلة ذكي خلال سنة أخيره قفز تطور تقني ذكاء اصطناعي قفز كبير عد تقني تعلم عميق أبرز مظهر هي ارتكز علي تطوير شبكة عصبي صناعي حاكى طريق عمل أسلوب دماغ بشري أن قادر علي تجريب تعلم تطوير نفس ذاتي دخل إنسان أثبت تقني تعلم عميق قدرة علي تعرف علي صور فهم كلام ترجم لغ إلى أخرى غير قدرة أغرى شركة أميركيه تحديد فيسبوك غوغل علي استثمار تكثيف بحث في متجاهلين تحذير تطور ذكاء اصطناعي هدد بشريه في آخر أشار عالم فيزياء راحل ستيفن هوكينغ إلى تطوير ذكاء اصطناعي كامل مهد فناء جنس بشري محذر قدر آلة علي إعاد تصميم نفس ذاتي أعلن مؤسس رئيس سابق شرك مايكروسوفت بيل غيتس رغبة بقاء روبوت غبي إلى حد ما قال معسكر شعر قلق ذكاء خارق في عام ذات وصف مخترع مستثمر أمريكي إيلون ماسك ذكاء اصطناعي أن أعظم مخاطر هدد وجود بشري تطوير آلة ذكيه استحضار شيطان استثمر ماسك مؤسس مشروع صاروخ فضاء تجاريه سبيس إكس سيارة تسلا كهربائيه غير مليون دولار بحث اكتشاف مخاطر محتمله ذكاء اصطناعي كيف تعامل مع مقابل يري خبير تقني ذكاء اصطناعي تسبب مخاطر علي جنس بشري أستاذ حاسوب جامع مونتريال كندي يوش بينغيو يري أن انبغى قلق تقني ذكيه هي احتاج سنة كثير تطور بطيء تدريجي وصل إلى مدي خشي محلل أن استند تطور إلى علم فكر زال بداية أول حالي أكد بينغيو وصول إلى ذكاء اصطناعي شكل منتظر كان مفاجئ أشبه بعض اكتشاف وصف سحري خارق علي حد تعبير ما زال إنتاج نظم ذكاء اصطناعي متكامله حاج إلى تطور علم حال ابتكار علم جديد تعبير آخر خرج عالم تقني ذكي شأن تغيير عالم ليل ضحى فيلم خيال علمي ناحي أخرى تقني ذكاء اصطناعي تطور سرع كبير أصبح تعقيد عام يري باحث متخصص مجال ذكاء اصطناعي لدى شرك غوغل جامع تورنتو جيوفري هينتون آلة وازى إنسان ذكاء خلال خمس عام وقف هينتون تطوير برنامج غوغل ذكي ألفاغو هزم بطل عالم لعب غو لكن يري على خشي ذكاء اصطناعي أن تقني جديد كان مثير خوف حال أساء استخدام قول أن مساله تعلق كيف تعامل تكنولوجيا شكل جعل مؤذي بشر عد شرك غوغل فيسبوك رائدة مجال تقني ذكاء اصطناعي ؛ هذا تقني ساعد غوغل علي تطوير خدمة شكل كبير مثل فضل تقني أمكن هاتف أندرويد هم أمر مستخدم ترجم فوريه عبارة مكتوبه بلغ أجنبي علي لافتة طريق أسهم تقني دعم محرك بحث غوغل عد أبرز منتج شرك نسب لفيسبوك سمح تعلم عميق شبكه اجتماعيه تعرف علي وجه صور اختيار محتوي مناسب عرض مستخدم علي صفح آخر خبر دعم مساعد شخصي رقمي تابع لفيسبوك إم غير وظيفة يري مدير تنفيذي شرك ألفابت شرك أم لغوغل سوندار يشاي عصر هاتف ذكيه اقترب نهاية استبدل ذكاء اصطناعي أتاح وصول فوري إلى معلومة ضروريه يري مؤسس فيسبوك رئيس تنفيذي أجهزه ذكاء اصطناعي استطاع يوم تمتع حاسة إنسانيه رؤيه شعور بشر نفس مصدر جزيره\n"
     ]
    }
   ],
   "source": [
    "print(\"Original Text :\")\n",
    "print(data[\"Text\"][2])\n",
    "print(\"*\"*100)\n",
    "print(\"processed_text : \")\n",
    "print(data[\"processed_text\"][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e274156c-1eaa-4ec8-b903-0ca2f97510e6",
   "metadata": {},
   "source": [
    "#### 3. Train your models by using RNN, Bidirectional RNN GRU and LSTM Architectures and tuning hyper-parameters to get the best performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "ac05a7c8-3f31-4bbd-a529-f9f50506c5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from nltk.tokenize import word_tokenize\n",
    "# from collections import Counter\n",
    "# from torchtext.vocab import build_vocab_from_iterator\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Example Arabic texts and labels\n",
    "texts = data[\"processed_text\"]\n",
    "scores = data[\"Score\"]  # Regression scores\n",
    "\n",
    "# Tokenize the texts\n",
    "tokenized_texts = [word_tokenize(text) for text in texts]\n",
    "\n",
    "# Train Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences=tokenized_texts, vector_size=1000, window=5, min_count=1, sg=1)\n",
    "\n",
    "# Convert texts to Word2Vec embeddings\n",
    "def texts_to_embeddings(texts, model, maxlen):\n",
    "    embeddings = []\n",
    "    for text in texts:\n",
    "        tokens = word_tokenize(text)\n",
    "        text_embedding = [model.wv[token] if token in model.wv else np.zeros(model.vector_size) for token in tokens]\n",
    "        if len(text_embedding) < maxlen:\n",
    "            text_embedding.extend([np.zeros(model.vector_size)] * (maxlen - len(text_embedding)))\n",
    "        else:\n",
    "            text_embedding = text_embedding[:maxlen]\n",
    "        embeddings.append(text_embedding)\n",
    "    return np.array(embeddings)\n",
    "\n",
    "# Set the maximum length for padding\n",
    "maxlen = 1500\n",
    "\n",
    "# Convert texts to embeddings and pad sequences\n",
    "data2 = texts_to_embeddings(texts, word2vec_model, maxlen)\n",
    "\n",
    "# Convert scores to numpy array\n",
    "scores = np.array(scores)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X = torch.tensor(data2, dtype=torch.float32)\n",
    "y = torch.tensor(scores, dtype=torch.float32)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "7c05bc91-c808-4a0b-8955-a43e3bdb1152",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the models\n",
    "class RNNRegressor(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim):\n",
    "        super(RNNRegressor, self).__init__()\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, _ = self.rnn(x)\n",
    "        output = self.fc(output[:, -1, :])\n",
    "        return output\n",
    "\n",
    "class BidirectionalRNNRegressor(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim):\n",
    "        super(BidirectionalRNNRegressor, self).__init__()\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, _ = self.rnn(x)\n",
    "        output = self.fc(output[:, -1, :])\n",
    "        return output\n",
    "\n",
    "class GRURegressor(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim):\n",
    "        super(GRURegressor, self).__init__()\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, _ = self.gru(x)\n",
    "        output = self.fc(output[:, -1, :])\n",
    "        return output\n",
    "\n",
    "class LSTMRegressor(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim):\n",
    "        super(LSTMRegressor, self).__init__()\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, _ = self.lstm(x)\n",
    "        output = self.fc(output[:, -1, :])\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "c52a3a79-b8d5-4eae-a5ec-e895feebe604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "embedding_dim = 1000  # Dimension of the word embeddings (same as Word2Vec vector size)\n",
    "hidden_dim = 500 #128  # Dimension of the hidden state\n",
    "\n",
    "# Initialize the models, loss functions, and optimizers\n",
    "rnn_model = RNNRegressor(embedding_dim, hidden_dim)\n",
    "bidirectional_rnn_model = BidirectionalRNNRegressor(embedding_dim, hidden_dim)\n",
    "gru_model = GRURegressor(embedding_dim, hidden_dim)\n",
    "lstm_model = LSTMRegressor(embedding_dim, hidden_dim)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer_rnn = optim.Adam(rnn_model.parameters(), lr=0.001)\n",
    "optimizer_bidirectional_rnn = optim.Adam(bidirectional_rnn_model.parameters(), lr=0.001)\n",
    "optimizer_gru = optim.Adam(gru_model.parameters(), lr=0.001)\n",
    "optimizer_lstm = optim.Adam(lstm_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "1df6691b-2987-4cb7-b862-4afb307fef5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] completed\n",
      "Epoch [2/10] completed\n",
      "Epoch [3/10] completed\n",
      "Epoch [4/10] completed\n",
      "Epoch [5/10] completed\n",
      "Epoch [6/10] completed\n",
      "Epoch [7/10] completed\n",
      "Epoch [8/10] completed\n",
      "Epoch [9/10] completed\n",
      "Epoch [10/10] completed\n"
     ]
    }
   ],
   "source": [
    "# Train the models\n",
    "num_epochs = 10\n",
    "batch_size = 5  # Adjust the batch size as needed\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, targets in train_loader:\n",
    "        # Train RNN model\n",
    "        optimizer_rnn.zero_grad()\n",
    "        outputs = rnn_model(inputs)\n",
    "        loss = criterion(outputs.squeeze(), targets)\n",
    "        loss.backward()\n",
    "        optimizer_rnn.step()\n",
    "\n",
    "        # Train Bidirectional RNN model\n",
    "        optimizer_bidirectional_rnn.zero_grad()\n",
    "        outputs = bidirectional_rnn_model(inputs)\n",
    "        loss = criterion(outputs.squeeze(), targets)\n",
    "        loss.backward()\n",
    "        optimizer_bidirectional_rnn.step()\n",
    "\n",
    "        # Train GRU model\n",
    "        optimizer_gru.zero_grad()\n",
    "        outputs = gru_model(inputs)\n",
    "        loss = criterion(outputs.squeeze(), targets)\n",
    "        loss.backward()\n",
    "        optimizer_gru.step()\n",
    "\n",
    "        # Train LSTM model\n",
    "        optimizer_lstm.zero_grad()\n",
    "        outputs = lstm_model(inputs)\n",
    "        loss = criterion(outputs.squeeze(), targets)\n",
    "        loss.backward()\n",
    "        optimizer_lstm.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}] completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a752d6f-f740-4a13-8b6c-6ad33a334aca",
   "metadata": {},
   "source": [
    "#### 4. Evaluate the four languages models by using standards metrics and other metrics like blue score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "3fbde837-7ad4-43b1-9f62-5e1960aafd5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "//////////////////////////////////////////////////\n",
      "RNN Mean Squared Error: 0.5741\n",
      "Bidirectional RNN Mean Squared Error: 1.5631\n",
      "GRU Mean Squared Error: 1.0088\n",
      "LSTM Mean Squared Error: 0.4177\n",
      "//////////////////////////////////////////////////\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the models\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_test)\n",
    "        mse = criterion(outputs.squeeze(), y_test)\n",
    "    model.train()\n",
    "    return mse.item()\n",
    "\n",
    "rnn_mse = evaluate_model(rnn_model, X_test, y_test)\n",
    "bidirectional_rnn_mse = evaluate_model(bidirectional_rnn_model, X_test, y_test)\n",
    "gru_mse = evaluate_model(gru_model, X_test, y_test)\n",
    "lstm_mse = evaluate_model(lstm_model, X_test, y_test)\n",
    "\n",
    "# Print results\n",
    "print('/'*50)\n",
    "print(f'RNN Mean Squared Error: {rnn_mse:.4f}')\n",
    "print(f'Bidirectional RNN Mean Squared Error: {bidirectional_rnn_mse:.4f}')\n",
    "print(f'GRU Mean Squared Error: {gru_mse:.4f}')\n",
    "print(f'LSTM Mean Squared Error: {lstm_mse:.4f}')\n",
    "print('/'*50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fab3555-d9c6-4d95-a155-395c4e84991d",
   "metadata": {},
   "source": [
    "-----------------\n",
    "## Part 2 - Transformer (Text generation) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a8753342-38f7-4856-a501-5d06d4665a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the GPT-2 Pre-trained Model\n",
    "import pandas as pd\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
    "import torch, json\n",
    "from transformers import GPT2LMHeadModel\n",
    "import torch.optim as optim\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.CRITICAL)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# use cuda if available to accelerate tensor operations by executing them on NVIDIA GPUs\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a80ecef4-dc75-4886-8770-e0a8ccb369e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model and tokenizer\n",
    "model_name = 'gpt2'\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c03afb6-e63b-4d6b-aa6e-0a749bce5426",
   "metadata": {},
   "source": [
    "##### we will gonna train this model on movies dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d484b87d-4c31-412a-a5d2-e02d7aa7aedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and Preprocess the Dataset\n",
    "df = pd.read_csv('tmdb_5000_movies.csv')\n",
    "# source  : https://www.kaggle.com/datasets/tmdb/tmdb-movie-metadata?select=tmdb_5000_movies.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7f636dbb-4c04-4970-95e6-b3c79ff9d38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only 200 line because the dataset is large and will take a long time to train the LLM\n",
    "df =df.head(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "45c2c419-58de-4eb0-90c1-d609bc3d5c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 200 entries, 0 to 199\n",
      "Data columns (total 20 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   budget                200 non-null    int64  \n",
      " 1   genres                200 non-null    object \n",
      " 2   homepage              153 non-null    object \n",
      " 3   id                    200 non-null    int64  \n",
      " 4   keywords              200 non-null    object \n",
      " 5   original_language     200 non-null    object \n",
      " 6   original_title        200 non-null    object \n",
      " 7   overview              200 non-null    object \n",
      " 8   popularity            200 non-null    float64\n",
      " 9   production_companies  200 non-null    object \n",
      " 10  production_countries  200 non-null    object \n",
      " 11  release_date          200 non-null    object \n",
      " 12  revenue               200 non-null    int64  \n",
      " 13  runtime               200 non-null    float64\n",
      " 14  spoken_languages      200 non-null    object \n",
      " 15  status                200 non-null    object \n",
      " 16  tagline               194 non-null    object \n",
      " 17  title                 200 non-null    object \n",
      " 18  vote_average          200 non-null    float64\n",
      " 19  vote_count            200 non-null    int64  \n",
      "dtypes: float64(3), int64(4), object(13)\n",
      "memory usage: 31.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d37eae4-4de3-4e8e-bc7f-c3126337fab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the 'overview', 'title', and 'keywords' fields\n",
    "overviews = df['overview'].tolist() \n",
    "titles = df['title'].tolist()\n",
    "keywords = df['keywords'].apply(lambda x: json.loads(x)).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7cf20012-7024-4b58-accc-555ccb1678f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': 1463, 'name': 'culture clash'}, {'id': 2964, 'name': 'future'}, {'id': 3386, 'name': 'space war'}, {'id': 3388, 'name': 'space colony'}, {'id': 3679, 'name': 'society'}, {'id': 3801, 'name': 'space travel'}, {'id': 9685, 'name': 'futuristic'}, {'id': 9840, 'name': 'romance'}, {'id': 9882, 'name': 'space'}, {'id': 9951, 'name': 'alien'}, {'id': 10148, 'name': 'tribe'}, {'id': 10158, 'name': 'alien planet'}, {'id': 10987, 'name': 'cgi'}, {'id': 11399, 'name': 'marine'}, {'id': 13065, 'name': 'soldier'}, {'id': 14643, 'name': 'battle'}, {'id': 14720, 'name': 'love affair'}, {'id': 165431, 'name': 'anti war'}, {'id': 193554, 'name': 'power relations'}, {'id': 206690, 'name': 'mind and soul'}, {'id': 209714, 'name': '3d'}]\n"
     ]
    }
   ],
   "source": [
    "print(keywords[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4b0ce6f-c1fa-430f-acb4-ebd5289ea07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract keyword names\n",
    "def extract_keyword_names(keyword_list):\n",
    "    return [keyword['name'] for keyword in keyword_list]\n",
    "\n",
    "keyword_names = [extract_keyword_names(kw) for kw in keywords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ac042a1-0691-4556-a3af-389cd661ea89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine overviews, titles, and keywords\n",
    "movie_data = [{'title': title, 'overview': overview, 'keywords': keyword_names} for title, overview, keyword_names in zip(titles, overviews, keyword_names)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4a338e99-a639-41b0-af09-4c9f8c4d9d33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>overview</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Avatar</td>\n",
       "      <td>In the 22nd century, a paraplegic Marine is di...</td>\n",
       "      <td>[culture clash, future, space war, space colon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pirates of the Caribbean: At World's End</td>\n",
       "      <td>Captain Barbossa, long believed to be dead, ha...</td>\n",
       "      <td>[ocean, drug abuse, exotic island, east india ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Spectre</td>\n",
       "      <td>A cryptic message from Bond’s past sends him o...</td>\n",
       "      <td>[spy, based on novel, secret agent, sequel, mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Dark Knight Rises</td>\n",
       "      <td>Following the death of District Attorney Harve...</td>\n",
       "      <td>[dc comics, crime fighter, terrorist, secret i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>John Carter</td>\n",
       "      <td>John Carter is a war-weary, former military ca...</td>\n",
       "      <td>[based on novel, mars, medallion, space travel...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      title  \\\n",
       "0                                    Avatar   \n",
       "1  Pirates of the Caribbean: At World's End   \n",
       "2                                   Spectre   \n",
       "3                     The Dark Knight Rises   \n",
       "4                               John Carter   \n",
       "\n",
       "                                            overview  \\\n",
       "0  In the 22nd century, a paraplegic Marine is di...   \n",
       "1  Captain Barbossa, long believed to be dead, ha...   \n",
       "2  A cryptic message from Bond’s past sends him o...   \n",
       "3  Following the death of District Attorney Harve...   \n",
       "4  John Carter is a war-weary, former military ca...   \n",
       "\n",
       "                                            keywords  \n",
       "0  [culture clash, future, space war, space colon...  \n",
       "1  [ocean, drug abuse, exotic island, east india ...  \n",
       "2  [spy, based on novel, secret agent, sequel, mi...  \n",
       "3  [dc comics, crime fighter, terrorist, secret i...  \n",
       "4  [based on novel, mars, medallion, space travel...  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(movie_data).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8726e74c-e010-4094-9165-63a53bbd8c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MovieDataset(Dataset):\n",
    "    def __init__(self, tokenizer, movie_data, block_size=512):\n",
    "        self.examples = []\n",
    "        self.tokenizer = tokenizer\n",
    "        self.block_size = block_size\n",
    "        \n",
    "        for data in movie_data:\n",
    "            title = data['title']\n",
    "            overview = data['overview']\n",
    "            keywords = list(data['keywords'])\n",
    "            keyword_string = \", \".join(keywords)  # Convert keywords to a comma-separated string\n",
    "            text = f\"movie name : {title} | overview : {overview} | Keywords : {keyword_string}\"  # Combine title, overview, and keywords\n",
    "            tokenized_text = tokenizer.encode(text, add_special_tokens=True, truncation=True, max_length=block_size)\n",
    "            self.examples.append(tokenized_text)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        return torch.tensor(self.examples[item], dtype=torch.long)\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        # Pad sequences to the maximum length in the batch\n",
    "        padded_batch = pad_sequence(batch, batch_first=True, padding_value=tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0)\n",
    "        return padded_batch\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = MovieDataset(tokenizer, movie_data)\n",
    "data_loader = DataLoader(dataset, sampler=RandomSampler(dataset), batch_size=2, collate_fn=dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "156a26c5-6d2b-4efd-819c-bddbb441d4da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([41364,  1438,  1058, 26703,   930, 16700,  1058,   554,   262,  2534,\n",
       "          358,  4289,    11,   257, 41406,  1455,   291, 11000,   318, 26562,\n",
       "          284,   262,  8824, 28518,   319,   257,  3748,  4365,    11,   475,\n",
       "         4329, 12445,  1022,  1708,  6266,   290, 10192,   281,  8756, 14355,\n",
       "           13,   930,  7383, 10879,  1058,  3968, 19122,    11,  2003,    11,\n",
       "         2272,  1175,    11,  2272, 18815,    11,  3592,    11,  2272,  3067,\n",
       "           11, 36701,    11, 19661,    11,  2272,    11,  8756,    11, 14893,\n",
       "           11,  8756,  5440,    11,   269, 12397,    11, 16050,    11, 10686,\n",
       "           11,  3344,    11,  1842, 14669,    11,  3098,  1175,    11,  1176,\n",
       "         2316,    11,  2000,   290,  5848,    11,   513,    67])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b2cec2e-ffbb-4876-a811-f0aeebe9deeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4803"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2cc6a1c4-bbf4-49c4-92b0-82fed845218b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 4.281391620635986\n",
      "Epoch: 0, Loss: 2.977623224258423\n",
      "Epoch: 0, Loss: 3.9310193061828613\n",
      "Epoch: 0, Loss: 3.681670665740967\n",
      "Epoch: 0, Loss: 3.6696484088897705\n",
      "Epoch: 0, Loss: 2.7895991802215576\n",
      "Epoch: 0, Loss: 2.7860119342803955\n",
      "Epoch: 0, Loss: 3.1522040367126465\n",
      "Epoch: 0, Loss: 3.1355981826782227\n",
      "Epoch: 0, Loss: 3.7260866165161133\n",
      "Epoch: 0, Loss: 3.2977912425994873\n",
      "Epoch: 0, Loss: 3.302785873413086\n",
      "Epoch: 0, Loss: 2.4102747440338135\n",
      "Epoch: 0, Loss: 2.958753824234009\n",
      "Epoch: 0, Loss: 3.6569931507110596\n",
      "Epoch: 0, Loss: 3.0554277896881104\n",
      "Epoch: 0, Loss: 3.113213539123535\n",
      "Epoch: 0, Loss: 2.4519002437591553\n",
      "Epoch: 0, Loss: 2.666919708251953\n",
      "Epoch: 0, Loss: 2.7734692096710205\n",
      "Epoch: 0, Loss: 3.0338475704193115\n",
      "Epoch: 0, Loss: 3.223327159881592\n",
      "Epoch: 0, Loss: 3.0406789779663086\n",
      "Epoch: 0, Loss: 3.259910821914673\n",
      "Epoch: 0, Loss: 2.7189362049102783\n",
      "Epoch: 0, Loss: 3.221954107284546\n",
      "Epoch: 0, Loss: 2.173658609390259\n",
      "Epoch: 0, Loss: 2.634922742843628\n",
      "Epoch: 0, Loss: 2.3968088626861572\n",
      "Epoch: 0, Loss: 2.9017410278320312\n",
      "Epoch: 0, Loss: 2.537061929702759\n",
      "Epoch: 0, Loss: 3.5846478939056396\n",
      "Epoch: 0, Loss: 3.1264102458953857\n",
      "Epoch: 0, Loss: 2.5608716011047363\n",
      "Epoch: 0, Loss: 2.8940017223358154\n",
      "Epoch: 0, Loss: 2.861722946166992\n",
      "Epoch: 0, Loss: 2.976043224334717\n",
      "Epoch: 0, Loss: 3.179232120513916\n",
      "Epoch: 0, Loss: 3.0118250846862793\n",
      "Epoch: 0, Loss: 2.3879551887512207\n",
      "Epoch: 0, Loss: 3.252439022064209\n",
      "Epoch: 0, Loss: 2.969501256942749\n",
      "Epoch: 0, Loss: 2.7189433574676514\n",
      "Epoch: 0, Loss: 3.1440589427948\n",
      "Epoch: 0, Loss: 2.405660629272461\n",
      "Epoch: 0, Loss: 2.557643413543701\n",
      "Epoch: 0, Loss: 2.782879114151001\n",
      "Epoch: 0, Loss: 2.2054548263549805\n",
      "Epoch: 0, Loss: 2.635897159576416\n",
      "Epoch: 0, Loss: 2.5838048458099365\n",
      "Epoch: 0, Loss: 2.621760606765747\n",
      "Epoch: 0, Loss: 3.0223841667175293\n",
      "Epoch: 0, Loss: 2.8278586864471436\n",
      "Epoch: 0, Loss: 2.980771064758301\n",
      "Epoch: 0, Loss: 3.185767650604248\n",
      "Epoch: 0, Loss: 2.3509843349456787\n",
      "Epoch: 0, Loss: 2.6361143589019775\n",
      "Epoch: 0, Loss: 2.785191297531128\n",
      "Epoch: 0, Loss: 2.558703899383545\n",
      "Epoch: 0, Loss: 2.866244077682495\n",
      "Epoch: 0, Loss: 3.5686235427856445\n",
      "Epoch: 0, Loss: 2.879596710205078\n",
      "Epoch: 0, Loss: 2.815239191055298\n",
      "Epoch: 0, Loss: 2.23360013961792\n",
      "Epoch: 0, Loss: 3.1615543365478516\n",
      "Epoch: 0, Loss: 2.802137613296509\n",
      "Epoch: 0, Loss: 2.690901041030884\n",
      "Epoch: 0, Loss: 2.817732334136963\n",
      "Epoch: 0, Loss: 2.5552978515625\n",
      "Epoch: 0, Loss: 2.6125435829162598\n",
      "Epoch: 0, Loss: 2.5536587238311768\n",
      "Epoch: 0, Loss: 2.188565731048584\n",
      "Epoch: 0, Loss: 2.460254430770874\n",
      "Epoch: 0, Loss: 2.7023820877075195\n",
      "Epoch: 0, Loss: 2.535142660140991\n",
      "Epoch: 0, Loss: 3.6091995239257812\n",
      "Epoch: 0, Loss: 2.747565746307373\n",
      "Epoch: 0, Loss: 2.7750062942504883\n",
      "Epoch: 0, Loss: 2.738571882247925\n",
      "Epoch: 0, Loss: 2.631824493408203\n",
      "Epoch: 0, Loss: 2.563251256942749\n",
      "Epoch: 0, Loss: 2.617499589920044\n",
      "Epoch: 0, Loss: 2.8197247982025146\n",
      "Epoch: 0, Loss: 3.264784574508667\n",
      "Epoch: 0, Loss: 2.1264195442199707\n",
      "Epoch: 0, Loss: 3.159170627593994\n",
      "Epoch: 0, Loss: 2.63112211227417\n",
      "Epoch: 0, Loss: 1.992180585861206\n",
      "Epoch: 0, Loss: 2.273324489593506\n",
      "Epoch: 0, Loss: 2.6636762619018555\n",
      "Epoch: 0, Loss: 2.7566299438476562\n",
      "Epoch: 0, Loss: 2.9350473880767822\n",
      "Epoch: 0, Loss: 2.838865280151367\n",
      "Epoch: 0, Loss: 2.4691390991210938\n",
      "Epoch: 0, Loss: 2.1338627338409424\n",
      "Epoch: 0, Loss: 2.7928476333618164\n",
      "Epoch: 0, Loss: 2.671156883239746\n",
      "Epoch: 0, Loss: 2.4933292865753174\n",
      "Epoch: 0, Loss: 2.784595489501953\n",
      "Epoch: 0, Loss: 3.023233652114868\n",
      "Epoch: 1, Loss: 2.2713067531585693\n",
      "Epoch: 1, Loss: 2.372323989868164\n",
      "Epoch: 1, Loss: 3.228630304336548\n",
      "Epoch: 1, Loss: 2.8397111892700195\n",
      "Epoch: 1, Loss: 2.6212003231048584\n",
      "Epoch: 1, Loss: 2.3430230617523193\n",
      "Epoch: 1, Loss: 2.4312078952789307\n",
      "Epoch: 1, Loss: 2.0633254051208496\n",
      "Epoch: 1, Loss: 2.41680908203125\n",
      "Epoch: 1, Loss: 2.0969290733337402\n",
      "Epoch: 1, Loss: 2.9696712493896484\n",
      "Epoch: 1, Loss: 2.729620933532715\n",
      "Epoch: 1, Loss: 2.5351264476776123\n",
      "Epoch: 1, Loss: 2.605452060699463\n",
      "Epoch: 1, Loss: 2.3662219047546387\n",
      "Epoch: 1, Loss: 3.3109633922576904\n",
      "Epoch: 1, Loss: 2.7368228435516357\n",
      "Epoch: 1, Loss: 1.432257890701294\n",
      "Epoch: 1, Loss: 2.470561981201172\n",
      "Epoch: 1, Loss: 2.90911865234375\n",
      "Epoch: 1, Loss: 2.748089551925659\n",
      "Epoch: 1, Loss: 2.08309268951416\n",
      "Epoch: 1, Loss: 2.585145950317383\n",
      "Epoch: 1, Loss: 2.0584630966186523\n",
      "Epoch: 1, Loss: 1.781044840812683\n",
      "Epoch: 1, Loss: 1.9343791007995605\n",
      "Epoch: 1, Loss: 2.7038204669952393\n",
      "Epoch: 1, Loss: 2.199880599975586\n",
      "Epoch: 1, Loss: 2.710191011428833\n",
      "Epoch: 1, Loss: 2.6007301807403564\n",
      "Epoch: 1, Loss: 2.419520854949951\n",
      "Epoch: 1, Loss: 2.5481185913085938\n",
      "Epoch: 1, Loss: 2.5359103679656982\n",
      "Epoch: 1, Loss: 2.4453001022338867\n",
      "Epoch: 1, Loss: 2.3780951499938965\n",
      "Epoch: 1, Loss: 2.8515329360961914\n",
      "Epoch: 1, Loss: 2.6426734924316406\n",
      "Epoch: 1, Loss: 2.5025744438171387\n",
      "Epoch: 1, Loss: 2.945467233657837\n",
      "Epoch: 1, Loss: 3.0137240886688232\n",
      "Epoch: 1, Loss: 2.938755512237549\n",
      "Epoch: 1, Loss: 2.3685715198516846\n",
      "Epoch: 1, Loss: 2.7808380126953125\n",
      "Epoch: 1, Loss: 2.121286392211914\n",
      "Epoch: 1, Loss: 2.2025058269500732\n",
      "Epoch: 1, Loss: 2.700112819671631\n",
      "Epoch: 1, Loss: 2.0714688301086426\n",
      "Epoch: 1, Loss: 1.8192903995513916\n",
      "Epoch: 1, Loss: 2.4149253368377686\n",
      "Epoch: 1, Loss: 2.857069253921509\n",
      "Epoch: 1, Loss: 3.23040509223938\n",
      "Epoch: 1, Loss: 2.4341673851013184\n",
      "Epoch: 1, Loss: 2.5666258335113525\n",
      "Epoch: 1, Loss: 2.2589221000671387\n",
      "Epoch: 1, Loss: 2.6881649494171143\n",
      "Epoch: 1, Loss: 2.731261968612671\n",
      "Epoch: 1, Loss: 2.724030017852783\n",
      "Epoch: 1, Loss: 2.6104509830474854\n",
      "Epoch: 1, Loss: 2.196798086166382\n",
      "Epoch: 1, Loss: 2.7616708278656006\n",
      "Epoch: 1, Loss: 2.4906957149505615\n",
      "Epoch: 1, Loss: 2.157907724380493\n",
      "Epoch: 1, Loss: 1.633831262588501\n",
      "Epoch: 1, Loss: 2.542558431625366\n",
      "Epoch: 1, Loss: 2.6600372791290283\n",
      "Epoch: 1, Loss: 2.1506569385528564\n",
      "Epoch: 1, Loss: 2.1864469051361084\n",
      "Epoch: 1, Loss: 2.3323893547058105\n",
      "Epoch: 1, Loss: 1.9416202306747437\n",
      "Epoch: 1, Loss: 2.805440664291382\n",
      "Epoch: 1, Loss: 2.651380777359009\n",
      "Epoch: 1, Loss: 1.8287104368209839\n",
      "Epoch: 1, Loss: 2.1066668033599854\n",
      "Epoch: 1, Loss: 2.438664436340332\n",
      "Epoch: 1, Loss: 2.512376546859741\n",
      "Epoch: 1, Loss: 2.6201000213623047\n",
      "Epoch: 1, Loss: 2.6789112091064453\n",
      "Epoch: 1, Loss: 2.3752682209014893\n",
      "Epoch: 1, Loss: 2.19286847114563\n",
      "Epoch: 1, Loss: 2.6479549407958984\n",
      "Epoch: 1, Loss: 2.618657350540161\n",
      "Epoch: 1, Loss: 2.2216787338256836\n",
      "Epoch: 1, Loss: 2.6694836616516113\n",
      "Epoch: 1, Loss: 2.127380847930908\n",
      "Epoch: 1, Loss: 2.4831385612487793\n",
      "Epoch: 1, Loss: 2.865682363510132\n",
      "Epoch: 1, Loss: 2.77946138381958\n",
      "Epoch: 1, Loss: 2.448638439178467\n",
      "Epoch: 1, Loss: 2.1932997703552246\n",
      "Epoch: 1, Loss: 2.2102386951446533\n",
      "Epoch: 1, Loss: 2.5273635387420654\n",
      "Epoch: 1, Loss: 2.3954579830169678\n",
      "Epoch: 1, Loss: 2.4198672771453857\n",
      "Epoch: 1, Loss: 2.1587657928466797\n",
      "Epoch: 1, Loss: 2.474672794342041\n",
      "Epoch: 1, Loss: 2.2440569400787354\n",
      "Epoch: 1, Loss: 1.8078664541244507\n",
      "Epoch: 1, Loss: 2.437065839767456\n",
      "Epoch: 1, Loss: 2.6421241760253906\n",
      "Epoch: 1, Loss: 2.1265175342559814\n",
      "Epoch: 2, Loss: 1.8925704956054688\n",
      "Epoch: 2, Loss: 2.6329233646392822\n",
      "Epoch: 2, Loss: 2.120163917541504\n",
      "Epoch: 2, Loss: 1.290588617324829\n",
      "Epoch: 2, Loss: 3.0203189849853516\n",
      "Epoch: 2, Loss: 2.1542370319366455\n",
      "Epoch: 2, Loss: 2.427004337310791\n",
      "Epoch: 2, Loss: 2.5977087020874023\n",
      "Epoch: 2, Loss: 2.0266902446746826\n",
      "Epoch: 2, Loss: 2.221618175506592\n",
      "Epoch: 2, Loss: 2.13252592086792\n",
      "Epoch: 2, Loss: 2.236698627471924\n",
      "Epoch: 2, Loss: 2.544409990310669\n",
      "Epoch: 2, Loss: 2.7992453575134277\n",
      "Epoch: 2, Loss: 2.0552847385406494\n",
      "Epoch: 2, Loss: 1.948007345199585\n",
      "Epoch: 2, Loss: 2.062931776046753\n",
      "Epoch: 2, Loss: 2.314893960952759\n",
      "Epoch: 2, Loss: 2.1824207305908203\n",
      "Epoch: 2, Loss: 2.5418190956115723\n",
      "Epoch: 2, Loss: 2.044185161590576\n",
      "Epoch: 2, Loss: 2.5570640563964844\n",
      "Epoch: 2, Loss: 2.0059118270874023\n",
      "Epoch: 2, Loss: 1.7224746942520142\n",
      "Epoch: 2, Loss: 2.4903712272644043\n",
      "Epoch: 2, Loss: 2.1612606048583984\n",
      "Epoch: 2, Loss: 1.9789425134658813\n",
      "Epoch: 2, Loss: 1.769479513168335\n",
      "Epoch: 2, Loss: 2.5487475395202637\n",
      "Epoch: 2, Loss: 2.4325754642486572\n",
      "Epoch: 2, Loss: 2.0133719444274902\n",
      "Epoch: 2, Loss: 2.3486359119415283\n",
      "Epoch: 2, Loss: 2.219928741455078\n",
      "Epoch: 2, Loss: 2.43420672416687\n",
      "Epoch: 2, Loss: 1.9839401245117188\n",
      "Epoch: 2, Loss: 2.0550804138183594\n",
      "Epoch: 2, Loss: 2.8731069564819336\n",
      "Epoch: 2, Loss: 2.529118776321411\n",
      "Epoch: 2, Loss: 2.331177234649658\n",
      "Epoch: 2, Loss: 2.536010265350342\n",
      "Epoch: 2, Loss: 2.0614428520202637\n",
      "Epoch: 2, Loss: 1.972306251525879\n",
      "Epoch: 2, Loss: 1.9973201751708984\n",
      "Epoch: 2, Loss: 2.491506814956665\n",
      "Epoch: 2, Loss: 2.0157382488250732\n",
      "Epoch: 2, Loss: 1.8283050060272217\n",
      "Epoch: 2, Loss: 2.4873034954071045\n",
      "Epoch: 2, Loss: 2.431391954421997\n",
      "Epoch: 2, Loss: 2.079714059829712\n",
      "Epoch: 2, Loss: 1.8776350021362305\n",
      "Epoch: 2, Loss: 2.760892391204834\n",
      "Epoch: 2, Loss: 2.108757972717285\n",
      "Epoch: 2, Loss: 2.754647731781006\n",
      "Epoch: 2, Loss: 2.749995708465576\n",
      "Epoch: 2, Loss: 1.841074824333191\n",
      "Epoch: 2, Loss: 2.2347960472106934\n",
      "Epoch: 2, Loss: 1.961279034614563\n",
      "Epoch: 2, Loss: 1.7382017374038696\n",
      "Epoch: 2, Loss: 3.024790048599243\n",
      "Epoch: 2, Loss: 2.0889639854431152\n",
      "Epoch: 2, Loss: 2.1973679065704346\n",
      "Epoch: 2, Loss: 2.6163530349731445\n",
      "Epoch: 2, Loss: 2.127331256866455\n",
      "Epoch: 2, Loss: 1.884264349937439\n",
      "Epoch: 2, Loss: 2.17895245552063\n",
      "Epoch: 2, Loss: 2.9656059741973877\n",
      "Epoch: 2, Loss: 2.464157819747925\n",
      "Epoch: 2, Loss: 2.3250205516815186\n",
      "Epoch: 2, Loss: 2.1248726844787598\n",
      "Epoch: 2, Loss: 2.1102137565612793\n",
      "Epoch: 2, Loss: 2.29321026802063\n",
      "Epoch: 2, Loss: 2.139953374862671\n",
      "Epoch: 2, Loss: 2.9088404178619385\n",
      "Epoch: 2, Loss: 2.0192883014678955\n",
      "Epoch: 2, Loss: 2.2640185356140137\n",
      "Epoch: 2, Loss: 2.5243818759918213\n",
      "Epoch: 2, Loss: 2.037174701690674\n",
      "Epoch: 2, Loss: 2.2239632606506348\n",
      "Epoch: 2, Loss: 2.201143741607666\n",
      "Epoch: 2, Loss: 2.6637940406799316\n",
      "Epoch: 2, Loss: 2.1604604721069336\n",
      "Epoch: 2, Loss: 2.336322069168091\n",
      "Epoch: 2, Loss: 2.589365005493164\n",
      "Epoch: 2, Loss: 2.2026984691619873\n",
      "Epoch: 2, Loss: 2.3844916820526123\n",
      "Epoch: 2, Loss: 2.1947243213653564\n",
      "Epoch: 2, Loss: 1.6224995851516724\n",
      "Epoch: 2, Loss: 2.4580070972442627\n",
      "Epoch: 2, Loss: 1.9130077362060547\n",
      "Epoch: 2, Loss: 2.671872138977051\n",
      "Epoch: 2, Loss: 2.4849002361297607\n",
      "Epoch: 2, Loss: 1.9834953546524048\n",
      "Epoch: 2, Loss: 1.8404234647750854\n",
      "Epoch: 2, Loss: 2.3527183532714844\n",
      "Epoch: 2, Loss: 2.595892906188965\n",
      "Epoch: 2, Loss: 2.186535358428955\n",
      "Epoch: 2, Loss: 2.2911453247070312\n",
      "Epoch: 2, Loss: 1.723126769065857\n",
      "Epoch: 2, Loss: 2.6321792602539062\n",
      "Epoch: 2, Loss: 2.4927778244018555\n"
     ]
    }
   ],
   "source": [
    "# Fine-tune the Model\n",
    "def train(model, data_loader, epochs=1):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "    total_steps = len(data_loader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for batch in data_loader:\n",
    "            inputs = batch.to(device)\n",
    "            outputs = model(inputs, labels=inputs)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            print(f\"Epoch: {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "# Load model\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Train the model\n",
    "train(model, data_loader, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "835a1b4e-68ae-4e38-8020-97a645ab6cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate text\n",
    "def generate_text(model, tokenizer, prompt, max_length=100):\n",
    "    model.eval()\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    input_ids = input_ids.to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=1,\n",
    "            no_repeat_ngram_size=2,\n",
    "            do_sample=True,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            temperature=0.7,\n",
    "        )\n",
    "        \n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99cc4b7-b567-4262-8c9e-2976f6086e7b",
   "metadata": {},
   "source": [
    "#### test the LLM before the training :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b0eb1e6b-3215-4680-a8d2-fc9a51456ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'avatar' movie, it's a true success.\n",
      "\n",
      "\"I am the first to admit that I am not the most talented director,\" she said. \"But I did it because I wanted to make a movie about women, and it is about the impact and beauty of women. It is not about trying to tell you how to be a feminist, but about how you can be the best you could be if you're a woman.\"\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "prompt = \"'avatar' movie\"\n",
    "generated_paragraph = generate_text(model, tokenizer, prompt)\n",
    "print(generated_paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5489b2db-e536-4e7b-92a1-f18d5bcbac1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overview about 'Pirates of the Caribbean' movie and the \"unholy grail\" of modern-day Hollywood, has been met with a wave of negative responses, which in turn have led to the release of a new movie, a \"Pirate of Hearts\" trailer, and a more recent interview with the star of The Avengers.\n",
      "\n",
      "After the trailer was released, many fans asked for more information about \"The Avengers,\" and when asked if they are going to see \"Captain America\n"
     ]
    }
   ],
   "source": [
    "prompt = \"overview about 'Pirates of the Caribbean' movie\"\n",
    "generated_paragraph = generate_text(model, tokenizer, prompt)\n",
    "print(generated_paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2c763d4f-936f-4f84-97ed-45f7f571aece",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inception movie, but not a movie.\n",
      "\n",
      "The movie came out on DVD with a DVD insert. It was a nice little thing. The film's music was nice and upbeat. I didn't really understand the movie's lyrics. And I was just in awe of the music. But I also liked the way the lyrics were written. When I heard the song, I thought, \"This is a great song.\"\n",
      " and I just loved the sound of it. You could tell the actors\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Inception movie\"\n",
    "generated_paragraph = generate_text(model, tokenizer, prompt)\n",
    "print(generated_paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "afa292f9-8fb4-47f6-9921-1c52acc4263f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movie.com/watch/8EzF9I3lNg — Michael Cohen (@MichaelCohen) July 16, 2016\n",
      "\n",
      "There were a couple of others that did not get much attention.\n",
      ".@Michael_Cohens: \"You know what? If I had my way, I'd like to have a little bit of a 'wow' moment with you. I don't know if it's a nice, emotional moment for you or not, but it will be\n"
     ]
    }
   ],
   "source": [
    "prompt = \"movie\"\n",
    "generated_paragraph = generate_text(model, tokenizer, prompt)\n",
    "print(generated_paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8bf4ff82-bc6d-4080-92a1-ce1442772530",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movie name=\"http://www.youtube.com/watch?v=rS5KX3u1qRQ&feature=youtu.be\" title=\"The Best of the Best\" ]\n",
      "\n",
      "\"The best of\" is a term used to describe the best performances by a person who has made it to the top of a list, or has won a Grammy for a performance.\n",
      ": Best Actor for the First Time. [1]\n",
      "- In a movie, a\n"
     ]
    }
   ],
   "source": [
    "prompt = \"movie name\"\n",
    "generated_paragraph = generate_text(model, tokenizer, prompt)\n",
    "print(generated_paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38cbfcb-858f-416e-8553-b22b177834a3",
   "metadata": {},
   "source": [
    "---------\n",
    "#### test the LLM after the training :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c84bce68-f533-4737-9cb6-4479cfcf30ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'avatar' movie is based on the novel by Alex Gibney, based around the fictional world of Snow White and the Huntsman. The film is set during the reign of King Gizzard and Her Men, and based upon the events of The Lord of the Rings trilogy. Gibneys novel is also based in the United Kingdom, where it is known as The Snow Queen.\n",
      "\n",
      "The film will be based loosely around Peter Jackson's Hobbit: The Battle of Five Armies, which is a\n"
     ]
    }
   ],
   "source": [
    "prompt = \"'avatar' movie\"\n",
    "generated_paragraph = generate_text(model, tokenizer, prompt)\n",
    "print(generated_paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e6a03717-cb28-4236-917d-7d840eb5a796",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overview about 'Pirates of the Caribbean' movie and its sequel. | Keywords : based on novel, pirate, adventure, film, based upon comic book, pixies, superhero, love of life, sequel, aftercreditsstinger, island, credits, duringcave, villain, whencameo, marvel cinematic universe, magic sword, 3d, director, dream, imax, remake, riddle, nameless hero, ghost, underwater, mermaid\n"
     ]
    }
   ],
   "source": [
    "prompt = \"overview about 'Pirates of the Caribbean' movie\"\n",
    "generated_paragraph = generate_text(model, tokenizer, prompt)\n",
    "print(generated_paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4aeaeab7-9773-4ec9-a633-0096d9a03c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inception movie: The Great Gatsby | overview: When a pair of gaggle over a cheeseburger and a couple of beers, the young couple is led to believe that the world has gone mad. The town has been hit by a massive explosion and the town is in danger of becoming a disaster. But the kids are not the only ones who are shaken by the chaos. They must find a way to save the village and save their friends. | Keywords : explosion, g\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Inception movie\"\n",
    "generated_paragraph = generate_text(model, tokenizer, prompt)\n",
    "print(generated_paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cd85717a-d6ac-4b0e-8029-63cf5c1a22ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movie name : The Hobbit: An Unexpected Journey | overview : Bilbo Baggins is an orphan. When his father, Gandalf, is killed by the evil Lord Sauron, Bilb has been left with nothing to live on. Gand is forced to find a new home, as his parents and brother leave him to become the Dwarves. | Keywords : son of the dragon, forest, adventure, hobbit, elves, dwarves, 3d, goblin, aftercreditsstinger\n"
     ]
    }
   ],
   "source": [
    "prompt = \"movie\"\n",
    "generated_paragraph = generate_text(model, tokenizer, prompt)\n",
    "print(generated_paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7a19e95f-2ce2-4e7a-babf-c758f8bbadee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movie name : X-Men: First Class | overview : Following the events of X: The Last Stand, a group of mutants from X, Magneto, Cyclops and Apocalypse go on a mission to stop the mutant invasion of Earth, which will change the course of the entire world. | Keywords : mutant, marvel comic, based on comic book, duringcreditsstinger, aftercrisis, sequel, mutant vs. villain, superhuman, supervillain, time travel, super speed\n"
     ]
    }
   ],
   "source": [
    "prompt = \"movie name\"\n",
    "generated_paragraph = generate_text(model, tokenizer, prompt)\n",
    "print(generated_paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f86f3778-fd33-4260-b877-39957a0099c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movie name : avatar | overview : A young boy with a crush on an avatar named Skye, the young avatar joins forces with his friend Skyler to help him get back to his former self. | Keywords : video games, video game, dream, 3d, avatar, animation, aftercreditsstinger, sequel, marvel cinematic universe, imax, germany, based on video arcade, whencameo!!!'!!.!!, 3D, game based upon\n"
     ]
    }
   ],
   "source": [
    "prompt = \"movie name : avatar\"\n",
    "generated_paragraph = generate_text(model, tokenizer, prompt)\n",
    "print(generated_paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0748da09-0f46-4373-a5f7-8eb1cb055ef4",
   "metadata": {},
   "source": [
    "----------\n",
    "## Part 3 : BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f544a4-82d8-4a0d-a3a5-05653ea35d33",
   "metadata": {},
   "source": [
    "##### Import the DataSet : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "5555d07a-51fc-4767-8936-89c475548da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas data frame : This code reads the data into a pandas data frame:\n",
    "import gzip\n",
    "\n",
    "def parse(path):\n",
    "  g = gzip.open(path, 'rb')\n",
    "  for l in g:\n",
    "    yield json.loads(l)\n",
    "\n",
    "def getDF(path):\n",
    "  i = 0\n",
    "  df = {}\n",
    "  for d in parse(path):\n",
    "    df[i] = d\n",
    "    i += 1\n",
    "  return pd.DataFrame.from_dict(df, orient='index')\n",
    "\n",
    "df = getDF('Magazine_Subscriptions_5.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "004ff71d-7cf7-4459-b801-03c1531bca82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>verified</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>vote</th>\n",
       "      <th>style</th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>True</td>\n",
       "      <td>02 26, 2014</td>\n",
       "      <td>A5QQOOZJOVPSF</td>\n",
       "      <td>B00005N7P0</td>\n",
       "      <td>John L. Mehlmauer</td>\n",
       "      <td>I'm old, and so is my computer.  Any advice th...</td>\n",
       "      <td>Cheapskates guide</td>\n",
       "      <td>1393372800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>False</td>\n",
       "      <td>03 6, 2004</td>\n",
       "      <td>A5RHZE7B8SV5Q</td>\n",
       "      <td>B00005N7PS</td>\n",
       "      <td>gorillazfan249</td>\n",
       "      <td>There's nothing to say, but if you want a REAL...</td>\n",
       "      <td>The best mature Men's magazine.</td>\n",
       "      <td>1078531200</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>07 15, 2003</td>\n",
       "      <td>A1RPTVW5VEOSI</td>\n",
       "      <td>B00005N7PS</td>\n",
       "      <td>Michael J. Edelman</td>\n",
       "      <td>If you're the kind of man who looks at himself...</td>\n",
       "      <td>THE Magazine for the Self-Centered Male</td>\n",
       "      <td>1058227200</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>01 31, 2015</td>\n",
       "      <td>A1SFRBCMW8XVBW</td>\n",
       "      <td>B00005N7PS</td>\n",
       "      <td>Hoyett L. Barnett</td>\n",
       "      <td>Nothing to it.  Just an advertisement.  Little...</td>\n",
       "      <td>Nothing to it. Just an advertisement. Little a...</td>\n",
       "      <td>1422662400</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>True</td>\n",
       "      <td>10 5, 2010</td>\n",
       "      <td>A1IU9VPCBKZPE8</td>\n",
       "      <td>B00005N7P0</td>\n",
       "      <td>Randolph Eck</td>\n",
       "      <td>When PC Magazine ceased publication of their p...</td>\n",
       "      <td>Excellent Computer Magazine</td>\n",
       "      <td>1286236800</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   overall  verified   reviewTime      reviewerID        asin  \\\n",
       "0      4.0      True  02 26, 2014   A5QQOOZJOVPSF  B00005N7P0   \n",
       "1      5.0     False   03 6, 2004   A5RHZE7B8SV5Q  B00005N7PS   \n",
       "2      1.0     False  07 15, 2003   A1RPTVW5VEOSI  B00005N7PS   \n",
       "3      1.0      True  01 31, 2015  A1SFRBCMW8XVBW  B00005N7PS   \n",
       "4      5.0      True   10 5, 2010  A1IU9VPCBKZPE8  B00005N7P0   \n",
       "\n",
       "         reviewerName                                         reviewText  \\\n",
       "0   John L. Mehlmauer  I'm old, and so is my computer.  Any advice th...   \n",
       "1      gorillazfan249  There's nothing to say, but if you want a REAL...   \n",
       "2  Michael J. Edelman  If you're the kind of man who looks at himself...   \n",
       "3   Hoyett L. Barnett  Nothing to it.  Just an advertisement.  Little...   \n",
       "4        Randolph Eck  When PC Magazine ceased publication of their p...   \n",
       "\n",
       "                                             summary  unixReviewTime vote  \\\n",
       "0                                  Cheapskates guide      1393372800  NaN   \n",
       "1                    The best mature Men's magazine.      1078531200    3   \n",
       "2            THE Magazine for the Self-Centered Male      1058227200   17   \n",
       "3  Nothing to it. Just an advertisement. Little a...      1422662400  NaN   \n",
       "4                        Excellent Computer Magazine      1286236800    2   \n",
       "\n",
       "  style image  \n",
       "0   NaN   NaN  \n",
       "1   NaN   NaN  \n",
       "2   NaN   NaN  \n",
       "3   NaN   NaN  \n",
       "4   NaN   NaN  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "5182ae45-7acd-4080-9389-d2ddd6f3bfc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2375 entries, 0 to 2374\n",
      "Data columns (total 12 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   overall         2375 non-null   float64\n",
      " 1   verified        2375 non-null   bool   \n",
      " 2   reviewTime      2375 non-null   object \n",
      " 3   reviewerID      2375 non-null   object \n",
      " 4   asin            2375 non-null   object \n",
      " 5   reviewerName    2375 non-null   object \n",
      " 6   reviewText      2374 non-null   object \n",
      " 7   summary         2373 non-null   object \n",
      " 8   unixReviewTime  2375 non-null   int64  \n",
      " 9   vote            569 non-null    object \n",
      " 10  style           1625 non-null   object \n",
      " 11  image           1 non-null      object \n",
      "dtypes: bool(1), float64(1), int64(1), object(9)\n",
      "memory usage: 225.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "91c634f9-eec8-4abb-a50b-f7ded5794bbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['overall', 'verified', 'reviewTime', 'reviewerID', 'asin',\n",
       "       'reviewerName', 'reviewText', 'summary', 'unixReviewTime', 'vote',\n",
       "       'style', 'image'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e94ba20-7c2d-42f6-abeb-3b3522fdc77f",
   "metadata": {},
   "source": [
    "#### 1. By using Pre-trained bert-base-uncased establish the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d61da4c7-24a6-470c-8930-f4419a1cbe64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load pre-trained model (weights)\n",
    "bert_model = BertForSequenceClassification.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "6cb5b954-bd4d-4f67-b095-1c9ea86abea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Select relevant columns and clean data\n",
    "df = df[['reviewText', 'overall']].dropna()\n",
    "df['reviewText'] = df['reviewText'].astype(str)\n",
    "\n",
    "df = df.head(500)\n",
    "\n",
    "# Split the dataset\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    df['reviewText'].tolist(),\n",
    "    df['overall'].tolist(),\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "116a9fd2-2a44-446a-b4e3-e4c47b4892cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I'm old, and so is my computer.  Any advice th...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>There's nothing to say, but if you want a REAL...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If you're the kind of man who looks at himself...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nothing to it.  Just an advertisement.  Little...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>When PC Magazine ceased publication of their p...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          reviewText  overall\n",
       "0  I'm old, and so is my computer.  Any advice th...      4.0\n",
       "1  There's nothing to say, but if you want a REAL...      5.0\n",
       "2  If you're the kind of man who looks at himself...      1.0\n",
       "3  Nothing to it.  Just an advertisement.  Little...      1.0\n",
       "4  When PC Magazine ceased publication of their p...      5.0"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "ccbbd607-27e6-4e6d-913b-d56765b30d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the data\n",
    "train_encodings = bert_tokenizer(train_texts, truncation=True, padding=True, max_length=512)\n",
    "test_encodings = bert_tokenizer(test_texts, truncation=True, padding=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "b47f29b1-a5d3-46eb-90ad-184e9cb72a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class MagazineSubscriptionsReviewsDS(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = MagazineSubscriptionsReviewsDS(train_encodings, train_labels)\n",
    "test_dataset = MagazineSubscriptionsReviewsDS(test_encodings, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "a06cca5f-51f2-4805-99d8-74c4257d5023",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'get_model_proto' from 'tensorflow.python.data.experimental.ops.iterator_ops' (C:\\Users\\driss\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\data\\experimental\\ops\\iterator_ops.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[140], line 30\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m     25\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m: acc,\n\u001b[0;32m     26\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m'\u001b[39m: f1,\n\u001b[0;32m     27\u001b[0m     }\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Initialize Trainer\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                        \u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                  \u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m         \u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m     40\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py:397\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[1;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[0;32m    395\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m=\u001b[39m args\n\u001b[0;32m    396\u001b[0m \u001b[38;5;66;03m# Seed must be set before instantiating the model when using model\u001b[39;00m\n\u001b[1;32m--> 397\u001b[0m enable_full_determinism(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mseed) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mfull_determinism \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mset_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    398\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhp_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeepspeed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer_utils.py:111\u001b[0m, in \u001b[0;36mset_seed\u001b[1;34m(seed, deterministic)\u001b[0m\n\u001b[0;32m    109\u001b[0m     torch\u001b[38;5;241m.\u001b[39mxpu\u001b[38;5;241m.\u001b[39mmanual_seed_all(seed)\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_tf_available():\n\u001b[1;32m--> 111\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m    113\u001b[0m     tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mset_seed(seed)\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m deterministic:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\__init__.py:53\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m autograph\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bitwise\n\u001b[1;32m---> 53\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compat\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\_api\\v2\\compat\\__init__.py:8\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.compat namespace\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m v1\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m v2\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m forward_compatibility_horizon \u001b[38;5;66;03m# line: 125\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\_api\\v2\\compat\\v1\\__init__.py:30\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m autograph\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bitwise\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compat\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\_api\\v2\\compat\\v1\\compat\\__init__.py:8\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.compat namespace\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m v1\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m v2\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m forward_compatibility_horizon \u001b[38;5;66;03m# line: 125\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\_api\\v2\\compat\\v1\\compat\\v1\\__init__.py:32\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compat\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m debugging\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\_api\\v2\\compat\\v1\\data\\__init__.py:8\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.data namespace\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m experimental\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AUTOTUNE \u001b[38;5;66;03m# line: 103\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DatasetV1 \u001b[38;5;28;01mas\u001b[39;00m Dataset \u001b[38;5;66;03m# line: 3677\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\_api\\v2\\compat\\v1\\data\\experimental\\__init__.py:32\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minterleave_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parallel_interleave \u001b[38;5;66;03m# line: 29\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minterleave_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sample_from_datasets_v1 \u001b[38;5;28;01mas\u001b[39;00m sample_from_datasets \u001b[38;5;66;03m# line: 158\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01miterator_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_model_proto \u001b[38;5;66;03m# line: 103\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01miterator_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_saveable_from_iterator \u001b[38;5;66;03m# line: 41\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlookup_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DatasetInitializer \u001b[38;5;66;03m# line: 54\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'get_model_proto' from 'tensorflow.python.data.experimental.ops.iterator_ops' (C:\\Users\\driss\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\data\\experimental\\ops\\iterator_ops.py)"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "bert_model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=5)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# Define compute_metrics function\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average='weighted')\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "    }\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=bert_model,                        \n",
    "    args=training_args,                  \n",
    "    train_dataset=train_dataset,         \n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "de8ffd2b-19f4-46b7-ae89-6ebaf8d175f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sentiment labels\n",
    "def get_sentiment_label(rating):\n",
    "    if rating >= 4:\n",
    "        return 2  # Positive\n",
    "    elif rating == 3:\n",
    "        return 1  # Neutral\n",
    "    else:\n",
    "        return 0  # Negative\n",
    "\n",
    "df['sentiment'] = df['overall'].apply(get_sentiment_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "7c9d484a-e3f5-4c33-8964-c92e663dd695",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I'm old, and so is my computer.  Any advice th...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>There's nothing to say, but if you want a REAL...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If you're the kind of man who looks at himself...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nothing to it.  Just an advertisement.  Little...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>When PC Magazine ceased publication of their p...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          reviewText  overall  sentiment\n",
       "0  I'm old, and so is my computer.  Any advice th...      4.0          2\n",
       "1  There's nothing to say, but if you want a REAL...      5.0          2\n",
       "2  If you're the kind of man who looks at himself...      1.0          0\n",
       "3  Nothing to it.  Just an advertisement.  Little...      1.0          0\n",
       "4  When PC Magazine ceased publication of their p...      5.0          2"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "141e7947-fe98-4b38-8591-c2dd4f8ee6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing reviewText or sentiment\n",
    "df = df.dropna(subset=['reviewText', 'sentiment'])\n",
    "\n",
    "# Split the dataset into train and validation sets\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert pandas DataFrame to Hugging Face Dataset\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "0a66c5e7-ed5f-46e1-bff2-971bede24c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|███████████████████████████████████████████████████████████████████| 399/399 [00:00<00:00, 606.14 examples/s]\n",
      "Map: 100%|███████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 510.21 examples/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Function to tokenize the text\n",
    "def tokenize_function(examples):\n",
    "    return bert_tokenizer(examples['reviewText'], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "# Tokenize the datasets\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Remove the original text column to avoid confusion during training\n",
    "train_dataset = train_dataset.remove_columns(['reviewText', '__index_level_0__'])\n",
    "val_dataset = val_dataset.remove_columns(['reviewText', '__index_level_0__'])\n",
    "\n",
    "# Load the pre-trained BERT model\n",
    "bert_model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "4c148213-5327-4a9f-bbb4-db7bc49add58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['DISABLE_TF'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "0dbd5ef3-a1df-4a2c-a3c4-e9cb2f6758fe",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'get_model_proto' from 'tensorflow.python.data.experimental.ops.iterator_ops' (C:\\Users\\driss\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\data\\experimental\\ops\\iterator_ops.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[125], line 13\u001b[0m\n\u001b[0;32m      2\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[0;32m      3\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./results\u001b[39m\u001b[38;5;124m'\u001b[39m,          \u001b[38;5;66;03m# Output directory\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     evaluation_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m,     \u001b[38;5;66;03m# Evaluation strategy to use\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m     weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m,               \u001b[38;5;66;03m# Strength of weight decay\u001b[39;00m\n\u001b[0;32m     10\u001b[0m )\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Create Trainer instance\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbert_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                         \u001b[49m\u001b[38;5;66;43;03m# The instantiated 🤗 Transformers model to be trained\u001b[39;49;00m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                  \u001b[49m\u001b[38;5;66;43;03m# Training arguments, defined above\u001b[39;49;00m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m         \u001b[49m\u001b[38;5;66;43;03m# Training dataset\u001b[39;49;00m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataset\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;66;43;03m# Evaluation dataset\u001b[39;49;00m\n\u001b[0;32m     18\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m     21\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py:397\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[1;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[0;32m    395\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m=\u001b[39m args\n\u001b[0;32m    396\u001b[0m \u001b[38;5;66;03m# Seed must be set before instantiating the model when using model\u001b[39;00m\n\u001b[1;32m--> 397\u001b[0m enable_full_determinism(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mseed) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mfull_determinism \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mset_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    398\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhp_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeepspeed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer_utils.py:111\u001b[0m, in \u001b[0;36mset_seed\u001b[1;34m(seed, deterministic)\u001b[0m\n\u001b[0;32m    109\u001b[0m     torch\u001b[38;5;241m.\u001b[39mxpu\u001b[38;5;241m.\u001b[39mmanual_seed_all(seed)\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_tf_available():\n\u001b[1;32m--> 111\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m    113\u001b[0m     tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mset_seed(seed)\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m deterministic:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\__init__.py:53\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m autograph\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bitwise\n\u001b[1;32m---> 53\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compat\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\_api\\v2\\compat\\__init__.py:8\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.compat namespace\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m v1\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m v2\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m forward_compatibility_horizon \u001b[38;5;66;03m# line: 125\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\_api\\v2\\compat\\v1\\__init__.py:30\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m autograph\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bitwise\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compat\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\_api\\v2\\compat\\v1\\compat\\__init__.py:8\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.compat namespace\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m v1\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m v2\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m forward_compatibility_horizon \u001b[38;5;66;03m# line: 125\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\_api\\v2\\compat\\v1\\compat\\v1\\__init__.py:32\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compat\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m debugging\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\_api\\v2\\compat\\v1\\data\\__init__.py:8\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.data namespace\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m experimental\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AUTOTUNE \u001b[38;5;66;03m# line: 103\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DatasetV1 \u001b[38;5;28;01mas\u001b[39;00m Dataset \u001b[38;5;66;03m# line: 3677\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\_api\\v2\\compat\\v1\\data\\experimental\\__init__.py:32\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minterleave_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parallel_interleave \u001b[38;5;66;03m# line: 29\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minterleave_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sample_from_datasets_v1 \u001b[38;5;28;01mas\u001b[39;00m sample_from_datasets \u001b[38;5;66;03m# line: 158\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01miterator_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_model_proto \u001b[38;5;66;03m# line: 103\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01miterator_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_saveable_from_iterator \u001b[38;5;66;03m# line: 41\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlookup_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DatasetInitializer \u001b[38;5;66;03m# line: 54\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'get_model_proto' from 'tensorflow.python.data.experimental.ops.iterator_ops' (C:\\Users\\driss\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\data\\experimental\\ops\\iterator_ops.py)"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # Output directory\n",
    "    evaluation_strategy=\"epoch\",     # Evaluation strategy to use\n",
    "    learning_rate=2e-5,              # Learning rate\n",
    "    per_device_train_batch_size=8,   # Batch size for training\n",
    "    per_device_eval_batch_size=8,    # Batch size for evaluation\n",
    "    num_train_epochs=3,              # Number of training epochs\n",
    "    weight_decay=0.01,               # Strength of weight decay\n",
    ")\n",
    "\n",
    "# Create Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=bert_model,                         # The instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # Training arguments, defined above\n",
    "    train_dataset=train_dataset,         # Training dataset\n",
    "    eval_dataset=val_dataset             # Evaluation dataset\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3f3573-e50e-42b8-8e46-8720f329f8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate the model\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee7d8ef-fed8-4fee-bba6-b7a0b224da27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae67d0f-dbdc-4071-af7a-9f11c772fc75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8459001-2c31-4ac6-a715-e4bc7d5bee41",
   "metadata": {},
   "source": [
    "## Synthese of the Lab: \n",
    "\n",
    "#### Part 1: Classification Regression\n",
    "\n",
    "1. **Data Collection**:\n",
    "   - Utilized web scraping tools like Scrapy and BeautifulSoup to collect Arabic text data on a specific topic.\n",
    "   - Created a dataset where each text is assigned a relevance score between 0 and 10.\n",
    "\n",
    "2. **Preprocessing Pipeline**:\n",
    "   - Performed text tokenization to split the text into individual tokens.\n",
    "   - Applied stemming and lemmatization to reduce words to their base or root form.\n",
    "   - Removed stop words to eliminate common words that do not contribute to the model's performance.\n",
    "   - Discretized scores to convert continuous relevance scores into discrete categories if necessary.\n",
    "\n",
    "3. **Model Training**:\n",
    "   - Trained four different models: RNN, Bidirectional RNN, GRU, and LSTM.\n",
    "   - Tuned hyperparameters such as learning rate, batch size, number of layers, and hidden units to optimize model performance.\n",
    "\n",
    "4. **Model Evaluation**:\n",
    "   - Evaluated models using standard metrics such as accuracy, precision, recall, F1 score, and custom metrics like the BLEU score.\n",
    "\n",
    "#### Part 2: Transformer (Text Generation)\n",
    "\n",
    "1. **Model Setup**:\n",
    "   - Installed the PyTorch-Transformers library and loaded the pre-trained GPT-2 model.\n",
    "\n",
    "2. **Fine-Tuning**:\n",
    "   - Fine-tuned the GPT-2 model on a custom dataset to adapt it to specific text generation tasks.\n",
    "\n",
    "3. **Text Generation**:\n",
    "   - Generated new paragraphs based on given sentences to test the fine-tuned model's text generation capabilities.\n",
    "\n",
    "#### Part 3: BERT\n",
    "\n",
    "1. **Data Preparation**:\n",
    "   - Used the Amazon review dataset provided (https://nijianmo.github.io/amazon/index.html).\n",
    "   - Prepared and preprocessed the data to be compatible with BERT input requirements.\n",
    "\n",
    "2. **Model Adaptation**:\n",
    "   - Adapted the BERT embedding layer to the dataset and fine-tuned the pre-trained BERT model.\n",
    "\n",
    "3. **Training and Hyperparameter Tuning**:\n",
    "   - Fine-tuned the BERT model with various hyperparameters to achieve optimal performance.\n",
    "\n",
    "4. **Model Evaluation**:\n",
    "   - Evaluated the model using standard metrics (accuracy, loss, F1 score) and specific metrics like BLEU score and BERT score.\n",
    "\n",
    "5. **Conclusion**:\n",
    "   - Summarized the effectiveness and efficiency of using pre-trained BERT models for text classification and regression tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284ec0f9-7557-4390-9f81-807acefa03d7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a2f7bb9c-9583-48b8-ba37-2177b1e678a2",
   "metadata": {},
   "source": [
    "## References : \n",
    "- https://www.geeksforgeeks.org/introduction-to-recurrent-neural-network/\n",
    "- https://www.geeksforgeeks.org/sentiment-analysis-with-an-recurrent-neural-networks-rnn/\n",
    "- https://www.geeksforgeeks.org/large-language-model-llm/\n",
    "- https://www.geeksforgeeks.org/getting-started-with-transformers/\n",
    "- https://www.geeksforgeeks.org/deep-learning-introduction-to-long-short-term-memory/\n",
    "- https://towardsdatascience.com/how-to-fine-tune-gpt-2-for-text-generation-ae2ea53bc272\n",
    "- https://www.datacamp.com/tutorial/fine-tuning-large-language-models\n",
    "- https://datagy.io/pytorch-tutorial/\n",
    "- https://www.learnpytorch.io/\n",
    "- "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
