{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f485743b-f1e5-4273-9ab1-f90f4c6a3081",
      "metadata": {
        "id": "f485743b-f1e5-4273-9ab1-f90f4c6a3081"
      },
      "source": [
        "## Performed by: **Idriss Khattabi** and Supervised by: **AACHAK Lotfi**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66303981",
      "metadata": {},
      "source": [
        "# Lab 2\n",
        "--------------\n",
        "## **Objective** : The main purpose behind this lab is to get familiar with NLP Rule based, Regex and NLP Word embedding."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bfde69c3-c17b-45a1-bce5-13e43a6ef723",
      "metadata": {
        "id": "bfde69c3-c17b-45a1-bce5-13e43a6ef723"
      },
      "source": [
        "## Part 1: Rule Based NLP and Regex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06a7be64-3817-4a4f-bfcb-c76c89323540",
      "metadata": {
        "id": "06a7be64-3817-4a4f-bfcb-c76c89323540",
        "outputId": "3b8a42cc-28b3-4c2e-8be4-0018a0067a07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Splited text: ['I bought three Samsung smartphones 150 $ ', 'four kilos of fresh banana for 1,2 dollar a kilogram', 'one Hamburger with 4,5 dollar.']\n",
            "cleaned_splited_text :  ['bought 3 samsung smartphones 150 $', '4 banana 1,2 kilogram', '1 hamburger 4,5']\n",
            "\n",
            "Product\t\t\tQuantity\tUnit Price\tTotal Price\n",
            "samsung smartphones      3           \t150         \t450.00\n",
            "banana                   4           \t1,2         \t4.80\n",
            "hamburger                1           \t4,5         \t4.50\n",
            "\n",
            "Total Cost: 459.30 dollars\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "words_to_int = {\"one\": 1, \"three\": 3, \"four\": 4}\n",
        "\n",
        "# Define my custom stopwords\n",
        "custom_stopwords = {\"dollar\", \"kilos\", \"dollar.\", \"fresh\"}\n",
        "\n",
        "def text_processing(text):\n",
        "    # convert to lowercases\n",
        "    text = text.lower()\n",
        "\n",
        "    # replace teh string number with intiger number\n",
        "    words = text.split()\n",
        "    replaced_text = \"\"\n",
        "    for word in words:\n",
        "        if word in words_to_int:\n",
        "            replaced_text += str(words_to_int[word]) + \" \"\n",
        "        else:\n",
        "            replaced_text += word + \" \"\n",
        "\n",
        "    text = replaced_text.strip()\n",
        "\n",
        "    # remove stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = text.split()\n",
        "    filtered_text = [word for word in words if word not in stop_words]\n",
        "    # remove my custom stopwords\n",
        "    filtered_text = [word for word in filtered_text if word not in custom_stopwords]\n",
        "    return ' '.join(filtered_text)\n",
        "\n",
        "def generate_bill(text):\n",
        "\n",
        "    # Splitting the text based on commas and \"and\"\n",
        "    splited_text = re.split(r'\\w*,\\s+| and ', text)\n",
        "\n",
        "    print(\"Splited text:\", splited_text)\n",
        "\n",
        "    cleaned_splited_text = [text_processing(split) for split in splited_text]\n",
        "    print(\"cleaned_splited_text : \", cleaned_splited_text)\n",
        "\n",
        "    print(\"\\nProduct\\t\\t\\tQuantity\\tUnit Price\\tTotal Price\")\n",
        "    total_cost = 0\n",
        "\n",
        "    for sentence in cleaned_splited_text:\n",
        "        match = re.search(r'(\\d+)\\s+(\\w+\\s*\\w*)\\s+(\\d+[,.]?\\d*)\\s*', sentence)\n",
        "        if match:\n",
        "            quantity, product, unit_price = match.groups()\n",
        "            total_price = float(quantity) * float(unit_price.replace(',', '.'))\n",
        "            total_cost += total_price\n",
        "            print(f\"{product.ljust(25, \" \")}{quantity.ljust(12, \" \")}\\t{unit_price.ljust(12, \" \")}\\t{total_price:.2f}\")\n",
        "\n",
        "    print(f\"\\nTotal Cost: {total_cost:.2f} dollars\")\n",
        "\n",
        "# Example usage\n",
        "text = \"I bought three Samsung smartphones 150 $ each, four kilos of fresh banana for 1,2 dollar a kilogram and one Hamburger with 4,5 dollar.\"\n",
        "generate_bill(text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "954ccf5f-4a83-4f14-9434-97cc0fa1acec",
      "metadata": {
        "id": "954ccf5f-4a83-4f14-9434-97cc0fa1acec"
      },
      "source": [
        "## Part 2 : word Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "d31a7e24-1e85-45a3-a279-289cca723dcc",
      "metadata": {
        "id": "d31a7e24-1e85-45a3-a279-289cca723dcc"
      },
      "outputs": [],
      "source": [
        "from pymongo import MongoClient\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "a23da01b-1e55-49e7-9c8d-6f14b20b624d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "a23da01b-1e55-49e7-9c8d-6f14b20b624d",
        "outputId": "c57c448b-6d79-4547-eee5-557a3bc92df8"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cleaned_posts</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ﺿﻤﻦ ﺍﻟﺘﻘﺮﻳﺮ ﺗﺴﻠﻤﻪ ﻋﺰﻳﺰ ﺃﺧﻨﻮﺵ، ﺭﺋﻴﺲ ﺍﻟﺤﻜﻮﻣﺔ، ﻧﻬ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ﻳﺘﺠﺪﺩ ﺍﻟﻨﻘﺎﺵ ﺃﻭﺳﺎﻁ ﺍﻟﻤﻬﺘﻤﻴﻦ ﺑﺸﺆﻭﻥ ﺍﻟﻤﺮﺃﺓ ﻭﺣﻘﻮﻗ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ﺃﻋﻠﻦ ﺷﻜﻴﺐ ﺑﻨﻤﻮﺳﻰ، ﻭﺯﻳﺮ ﺍﻟﺘﺮﺑﻴﺔ ﺍﻟﻮﻃﻨﻴﺔ ﻭﺍﻟﺘﻌﻠﻴ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ﺗﺘﺠﻪ ﺃﻧﻈﺎﺭ ﺷﺒﻜﺎﺕ ﺍﻟﻬﺠﺮﺓ ﺍﻟﺴﺮﻳﺔ ﺑﺎﻟﻤﻐﺮﺏ ﻣﻮﻋﺪ ﻋﻴ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ﺗﻮﺻﻠﺖ ﺟﺮﻳﺪﺓ ﻫﺴﺒﺮﻳﺲ ﺍﻹﻟﻜﺘﺮﻭﻧﻴﺔ ﺑﺮﺩ ﺟﻤﻌﻴﺔ ﺍﻷﻋﻤﺎﻝ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                       cleaned_posts\n",
              "0  ﺿﻤﻦ ﺍﻟﺘﻘﺮﻳﺮ ﺗﺴﻠﻤﻪ ﻋﺰﻳﺰ ﺃﺧﻨﻮﺵ، ﺭﺋﻴﺲ ﺍﻟﺤﻜﻮﻣﺔ، ﻧﻬ...\n",
              "1  ﻳﺘﺠﺪﺩ ﺍﻟﻨﻘﺎﺵ ﺃﻭﺳﺎﻁ ﺍﻟﻤﻬﺘﻤﻴﻦ ﺑﺸﺆﻭﻥ ﺍﻟﻤﺮﺃﺓ ﻭﺣﻘﻮﻗ...\n",
              "2  ﺃﻋﻠﻦ ﺷﻜﻴﺐ ﺑﻨﻤﻮﺳﻰ، ﻭﺯﻳﺮ ﺍﻟﺘﺮﺑﻴﺔ ﺍﻟﻮﻃﻨﻴﺔ ﻭﺍﻟﺘﻌﻠﻴ...\n",
              "3  ﺗﺘﺠﻪ ﺃﻧﻈﺎﺭ ﺷﺒﻜﺎﺕ ﺍﻟﻬﺠﺮﺓ ﺍﻟﺴﺮﻳﺔ ﺑﺎﻟﻤﻐﺮﺏ ﻣﻮﻋﺪ ﻋﻴ...\n",
              "4  ﺗﻮﺻﻠﺖ ﺟﺮﻳﺪﺓ ﻫﺴﺒﺮﻳﺲ ﺍﻹﻟﻜﺘﺮﻭﻧﻴﺔ ﺑﺮﺩ ﺟﻤﻌﻴﺔ ﺍﻷﻋﻤﺎﻝ..."
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# First, load the data from MongoDB\n",
        "\n",
        "# Connect to MongoDB server\n",
        "client = MongoClient('mongodb://localhost:27017/')\n",
        "\n",
        "# Select the database\n",
        "db = client['NLP_DB1']\n",
        "\n",
        "# Select the collection\n",
        "collection = db['cleaned_hespress_posts']\n",
        "\n",
        "projection = {'_id': 0, 'cleaned_posts': 1}\n",
        "\n",
        "# Query the collection to retrieve documents\n",
        "all_posts = collection.find({}, projection)\n",
        "\n",
        "# Convert the data to pandas's DataFrame\n",
        "all_posts = pd.DataFrame(list(all_posts))\n",
        "\n",
        "all_posts.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "3vUmvSxMdesF",
      "metadata": {
        "id": "3vUmvSxMdesF"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "QONp2t3ldfvY",
      "metadata": {
        "id": "QONp2t3ldfvY"
      },
      "outputs": [],
      "source": [
        "post_tokens = []\n",
        "\n",
        "# tokenize each text\n",
        "for i in range(len(all_posts)):\n",
        "    tokens = word_tokenize(all_posts[\"cleaned_data\"][i])\n",
        "    post_tokens.append(tokens)\n",
        "\n",
        "all_posts[\"tokens\"] = post_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22daf2bb-c171-40aa-8a88-4f90164a07c9",
      "metadata": {
        "id": "22daf2bb-c171-40aa-8a88-4f90164a07c9"
      },
      "source": [
        "### I alredy apply on this data : cleaning the text, tokenization, Remove stop words, normalize the text and Lemmatization, from the lab 1."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "430b20dc-3e41-49ad-9e50-53b4bf4f1f6e",
      "metadata": {
        "id": "430b20dc-3e41-49ad-9e50-53b4bf4f1f6e"
      },
      "source": [
        "### 1. Apply one hot encoding, bag of words, TF-IDF technics on the Data vectors collected during the lab 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8e5879c-c53f-4990-9bab-c63e159939c2",
      "metadata": {
        "id": "e8e5879c-c53f-4990-9bab-c63e159939c2"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "728af7a6-84f8-405e-b1c1-c9a433ebf268",
      "metadata": {
        "id": "728af7a6-84f8-405e-b1c1-c9a433ebf268"
      },
      "outputs": [],
      "source": [
        "post_tokens = []\n",
        "\n",
        "# tokenize each text\n",
        "for i in range(len(all_posts)):\n",
        "    tokens = word_tokenize(all_posts[\"cleaned_posts\"][i])\n",
        "    post_tokens.append(tokens)\n",
        "\n",
        "all_posts[\"tokens\"] = post_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d16596e-ddda-438a-b912-46004cec270c",
      "metadata": {
        "id": "6d16596e-ddda-438a-b912-46004cec270c",
        "outputId": "92449c46-a281-45a3-e770-b62c7bf605d5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cleaned_posts</th>\n",
              "      <th>tokens</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ﺿﻤﻦ ﺍﻟﺘﻘﺮﻳﺮ ﺗﺴﻠﻤﻪ ﻋﺰﻳﺰ ﺃﺧﻨﻮﺵ، ﺭﺋﻴﺲ ﺍﻟﺤﻜﻮﻣﺔ، ﻧﻬ...</td>\n",
              "      <td>[ﺿﻤﻦ, ﺍﻟﺘﻘﺮﻳﺮ, ﺗﺴﻠﻤﻪ, ﻋﺰﻳﺰ, ﺃﺧﻨﻮﺵ،, ﺭﺋﻴﺲ, ﺍﻟﺤﻜ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ﻳﺘﺠﺪﺩ ﺍﻟﻨﻘﺎﺵ ﺃﻭﺳﺎﻁ ﺍﻟﻤﻬﺘﻤﻴﻦ ﺑﺸﺆﻭﻥ ﺍﻟﻤﺮﺃﺓ ﻭﺣﻘﻮﻗ...</td>\n",
              "      <td>[ﻳﺘﺠﺪﺩ, ﺍﻟﻨﻘﺎﺵ, ﺃﻭﺳﺎﻁ, ﺍﻟﻤﻬﺘﻤﻴﻦ, ﺑﺸﺆﻭﻥ, ﺍﻟﻤﺮﺃﺓ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ﺃﻋﻠﻦ ﺷﻜﻴﺐ ﺑﻨﻤﻮﺳﻰ، ﻭﺯﻳﺮ ﺍﻟﺘﺮﺑﻴﺔ ﺍﻟﻮﻃﻨﻴﺔ ﻭﺍﻟﺘﻌﻠﻴ...</td>\n",
              "      <td>[ﺃﻋﻠﻦ, ﺷﻜﻴﺐ, ﺑﻨﻤﻮﺳﻰ،, ﻭﺯﻳﺮ, ﺍﻟﺘﺮﺑﻴﺔ, ﺍﻟﻮﻃﻨﻴﺔ, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ﺗﺘﺠﻪ ﺃﻧﻈﺎﺭ ﺷﺒﻜﺎﺕ ﺍﻟﻬﺠﺮﺓ ﺍﻟﺴﺮﻳﺔ ﺑﺎﻟﻤﻐﺮﺏ ﻣﻮﻋﺪ ﻋﻴ...</td>\n",
              "      <td>[ﺗﺘﺠﻪ, ﺃﻧﻈﺎﺭ, ﺷﺒﻜﺎﺕ, ﺍﻟﻬﺠﺮﺓ, ﺍﻟﺴﺮﻳﺔ, ﺑﺎﻟﻤﻐﺮﺏ, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ﺗﻮﺻﻠﺖ ﺟﺮﻳﺪﺓ ﻫﺴﺒﺮﻳﺲ ﺍﻹﻟﻜﺘﺮﻭﻧﻴﺔ ﺑﺮﺩ ﺟﻤﻌﻴﺔ ﺍﻷﻋﻤﺎﻝ...</td>\n",
              "      <td>[ﺗﻮﺻﻠﺖ, ﺟﺮﻳﺪﺓ, ﻫﺴﺒﺮﻳﺲ, ﺍﻹﻟﻜﺘﺮﻭﻧﻴﺔ, ﺑﺮﺩ, ﺟﻤﻌﻴﺔ,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                       cleaned_posts  \\\n",
              "0  ﺿﻤﻦ ﺍﻟﺘﻘﺮﻳﺮ ﺗﺴﻠﻤﻪ ﻋﺰﻳﺰ ﺃﺧﻨﻮﺵ، ﺭﺋﻴﺲ ﺍﻟﺤﻜﻮﻣﺔ، ﻧﻬ...   \n",
              "1  ﻳﺘﺠﺪﺩ ﺍﻟﻨﻘﺎﺵ ﺃﻭﺳﺎﻁ ﺍﻟﻤﻬﺘﻤﻴﻦ ﺑﺸﺆﻭﻥ ﺍﻟﻤﺮﺃﺓ ﻭﺣﻘﻮﻗ...   \n",
              "2  ﺃﻋﻠﻦ ﺷﻜﻴﺐ ﺑﻨﻤﻮﺳﻰ، ﻭﺯﻳﺮ ﺍﻟﺘﺮﺑﻴﺔ ﺍﻟﻮﻃﻨﻴﺔ ﻭﺍﻟﺘﻌﻠﻴ...   \n",
              "3  ﺗﺘﺠﻪ ﺃﻧﻈﺎﺭ ﺷﺒﻜﺎﺕ ﺍﻟﻬﺠﺮﺓ ﺍﻟﺴﺮﻳﺔ ﺑﺎﻟﻤﻐﺮﺏ ﻣﻮﻋﺪ ﻋﻴ...   \n",
              "4  ﺗﻮﺻﻠﺖ ﺟﺮﻳﺪﺓ ﻫﺴﺒﺮﻳﺲ ﺍﻹﻟﻜﺘﺮﻭﻧﻴﺔ ﺑﺮﺩ ﺟﻤﻌﻴﺔ ﺍﻷﻋﻤﺎﻝ...   \n",
              "\n",
              "                                              tokens  \n",
              "0  [ﺿﻤﻦ, ﺍﻟﺘﻘﺮﻳﺮ, ﺗﺴﻠﻤﻪ, ﻋﺰﻳﺰ, ﺃﺧﻨﻮﺵ،, ﺭﺋﻴﺲ, ﺍﻟﺤﻜ...  \n",
              "1  [ﻳﺘﺠﺪﺩ, ﺍﻟﻨﻘﺎﺵ, ﺃﻭﺳﺎﻁ, ﺍﻟﻤﻬﺘﻤﻴﻦ, ﺑﺸﺆﻭﻥ, ﺍﻟﻤﺮﺃﺓ...  \n",
              "2  [ﺃﻋﻠﻦ, ﺷﻜﻴﺐ, ﺑﻨﻤﻮﺳﻰ،, ﻭﺯﻳﺮ, ﺍﻟﺘﺮﺑﻴﺔ, ﺍﻟﻮﻃﻨﻴﺔ, ...  \n",
              "3  [ﺗﺘﺠﻪ, ﺃﻧﻈﺎﺭ, ﺷﺒﻜﺎﺕ, ﺍﻟﻬﺠﺮﺓ, ﺍﻟﺴﺮﻳﺔ, ﺑﺎﻟﻤﻐﺮﺏ, ...  \n",
              "4  [ﺗﻮﺻﻠﺖ, ﺟﺮﻳﺪﺓ, ﻫﺴﺒﺮﻳﺲ, ﺍﻹﻟﻜﺘﺮﻭﻧﻴﺔ, ﺑﺮﺩ, ﺟﻤﻌﻴﺔ,...  "
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "all_posts.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "031edc07-236e-4d6f-89e1-37c9a50772cf",
      "metadata": {
        "id": "031edc07-236e-4d6f-89e1-37c9a50772cf",
        "outputId": "ae847011-07b4-4630-db8b-c42c3de9dfd5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['ﺿﻤﻦ', 'ﺍﻟﺘﻘﺮﻳﺮ', 'ﺗﺴﻠﻤﻪ', 'ﻋﺰﻳﺰ', 'ﺃﺧﻨﻮﺵ،', 'ﺭﺋﻴﺲ', 'ﺍﻟﺤﻜﻮﻣﺔ،', 'ﻧﻬﺎﻳﺔ', 'ﺷﻬﺮ', 'ﺍﻟﻤﺎﺿﻲ،', 'ﺍﻟﻬﻴﺌﺔ', 'ﺍﻟﻤﻜﻠﻔﺔ', 'ﺑﻤﺮﺍﺟﻌﺔ', 'ﻣﺪﻭﻧﺔ', 'ﺍﻷﺳﺮﺓ،', 'ﺍﻟﻤﻘﺮﺭ', 'ﺭﻓﻌﻪ', 'ﺍﻟﻤﻠﻚ', 'ﻣﺤﻤﺪ', 'ﺍﻟﺴﺎﺩﺱ،', 'ﻻﻓﺘﺎ', 'ﻭﺟﻮﺩ', 'ﻣﻘﺘﺮﺡ', 'ﺗﻄﺮﻕ', 'ﺗﻘﺴﻴﻢ', 'ﺍﻟﺘﺮﻛﺔ', 'ﺧﻼﻝ', 'ﺗﺨﻮﻳﻞ', 'ﺻﺎﺣﺐ', 'ﺍﻟﻤﺎﻝ', 'ﻭﺍﻷﺻﻮﻝ', 'ﺳﻠﻄﺔ', 'ﺍﺧﺘﻴﺎﺭ', 'ﺍﻟﻨﻈﺎﻡ', 'ﺍﻟﻤﻄﺒﻖ،', 'ﺍﻟﻤﻴﺮﺍﺙ', 'ﺍﻟﻮﺻﻴﺔ،', 'ﺗﻮﺳﻴﻊ', 'ﻧﻄﺎﻕ', 'ﺍﻷﺧﻴﺮﺓ', 'ﻟﺘﺸﻤﻞ', 'ﺍﻷﺣﻔﺎﺩ', 'ﻭﺭﻓﻊ', 'ﺍﻟﻘﻴﻮﺩ', 'ﺍﻟﻤﻔﺮﻭﺿﺔ', 'ﻋﻠﻴﻬﺎ', 'ﻭﻋﻠﻰ', 'ﺍﻟﺮﻏﻢ', 'ﺍﻟﻤﻘﺘﺮﺡ،', 'ﻋﺪﻩ']\n"
          ]
        }
      ],
      "source": [
        "print(post_tokens[0][:50])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c420f7f-6a83-4b2e-9ead-e881246bc215",
      "metadata": {
        "id": "0c420f7f-6a83-4b2e-9ead-e881246bc215"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0a20872-3f43-4bf6-973c-9d9b3ee387fb",
      "metadata": {
        "id": "d0a20872-3f43-4bf6-973c-9d9b3ee387fb"
      },
      "source": [
        "#### 1. One-Hot Encoding:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db915168-25b2-4f36-849e-30db7118c36a",
      "metadata": {
        "id": "db915168-25b2-4f36-849e-30db7118c36a"
      },
      "source": [
        "* test :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03a1c5f1-b7f7-461c-a153-9ee62eee2428",
      "metadata": {
        "id": "03a1c5f1-b7f7-461c-a153-9ee62eee2428",
        "outputId": "05db1001-718e-433c-9447-16bc7bc11fb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ﺿﻤﻦ : [0. 0. 0. 0. 0. 1. 0.]\n",
            "ﺍﻟﺘﻘﺮﻳﺮ : [0. 0. 0. 0. 1. 0. 0.]\n",
            "ﺗﺴﻠﻤﻪ : [0. 0. 1. 0. 0. 0. 0.]\n",
            "ﻋﺰﻳﺰ : [0. 0. 0. 0. 0. 0. 1.]\n",
            "ﺃﺧﻨﻮﺵ، : [0. 0. 0. 1. 0. 0. 0.]\n",
            "ﺭﺋﻴﺲ : [0. 1. 0. 0. 0. 0. 0.]\n",
            "ﺍﻟﺤﻜﻮﻣﺔ، : [1. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ],
      "source": [
        "test_tokens = ['ﺿﻤﻦ', 'ﺍﻟﺘﻘﺮﻳﺮ', 'ﺗﺴﻠﻤﻪ', 'ﻋﺰﻳﺰ', 'ﺃﺧﻨﻮﺵ،', 'ﺭﺋﻴﺲ', 'ﺍﻟﺤﻜﻮﻣﺔ،']\n",
        "\n",
        "token_indices2 = {token: i for i, token in enumerate(set(test_tokens))}\n",
        "encoded_tokens2 = [[token_indices2[token]] for token in test_tokens]\n",
        "encoded_tokens_array2 = np.array(encoded_tokens2)\n",
        "encoded_tokens_array2 = encoded_tokens_array2.reshape(-1, 1)\n",
        "encoder2 = OneHotEncoder()\n",
        "one_hot_encoded2 = encoder2.fit_transform(encoded_tokens_array2).toarray()\n",
        "for token, encoded in zip(test_tokens, one_hot_encoded2):\n",
        "    print(f\"{token} : {encoded}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6011b88f-2aef-4560-bc21-1771e8683069",
      "metadata": {
        "id": "6011b88f-2aef-4560-bc21-1771e8683069"
      },
      "source": [
        "* apply on real data :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fae2229-ad99-484a-bbb6-51884d45238c",
      "metadata": {
        "id": "4fae2229-ad99-484a-bbb6-51884d45238c",
        "outputId": "f5b7f4f7-2575-4cf5-a361-76f02ae4b05a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "display the encodage (One-Hot Encoded) of the first '3' tokens : \n",
            "ﺿﻤﻦ: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0.]\n",
            "ﺍﻟﺘﻘﺮﻳﺮ: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0.]\n",
            "ﺗﺴﻠﻤﻪ: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ],
      "source": [
        "all_one_hot_encoded = []\n",
        "\n",
        "# tokenize each text\n",
        "for i in range(len(all_posts[\"tokens\"])):\n",
        "\n",
        "    tokens = all_posts[\"tokens\"][i]\n",
        "\n",
        "    # Convert tokens to indices\n",
        "    token_indices = {token: i for i, token in enumerate(set(tokens))}\n",
        "    encoded_tokens = [[token_indices[token]] for token in tokens]\n",
        "\n",
        "    # Convert the 2D list into a 2D NumPy array\n",
        "    encoded_tokens_array = np.array(encoded_tokens)\n",
        "\n",
        "    # Reshape the array to be 2D\n",
        "    encoded_tokens_array = encoded_tokens_array.reshape(-1, 1)\n",
        "\n",
        "    # Apply one-hot encoding\n",
        "    encoder = OneHotEncoder()\n",
        "    one_hot_encoded = encoder.fit_transform(encoded_tokens_array).toarray()\n",
        "    all_one_hot_encoded.append(one_hot_encoded)\n",
        "\n",
        "# display the encodage of the first 5 tokens\n",
        "print(\"display the encodage (One-Hot Encoded) of the first '3' tokens : \")\n",
        "for token, encoded in zip(post_tokens[0][:3], all_one_hot_encoded[0][:3]):\n",
        "    print(f\"{token}: {encoded}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f037c6ce-2e79-4065-bbd2-30d04feffbe2",
      "metadata": {
        "id": "f037c6ce-2e79-4065-bbd2-30d04feffbe2"
      },
      "source": [
        "#### 2. Bag of Words:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67d7205e-1a72-4d35-86f5-385485e77888",
      "metadata": {
        "id": "67d7205e-1a72-4d35-86f5-385485e77888"
      },
      "source": [
        "* test :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f190622-2d41-41df-b599-ef1921b6e467",
      "metadata": {
        "id": "0f190622-2d41-41df-b599-ef1921b6e467",
        "outputId": "caeeb06c-f943-4b2b-83f3-e81e64f86a9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Bag of Words:\n",
            "ﺿﻤﻦ: 1\n",
            "ﺍﻟﺘﻘﺮﻳﺮ: 1\n",
            "ﺗﺴﻠﻤﻪ: 2\n",
            "ﻋﺰﻳﺰ: 1\n",
            "ﺃﺧﻨﻮﺵ: 2\n",
            "ﺭﺋﻴﺲ: 1\n",
            "ﺍﻟﺤﻜﻮﻣﺔ: 1\n"
          ]
        }
      ],
      "source": [
        "test_tokens = ['ﺿﻤﻦ', 'ﺍﻟﺘﻘﺮﻳﺮ', 'ﺗﺴﻠﻤﻪ', 'ﻋﺰﻳﺰ', 'ﺃﺧﻨﻮﺵ', 'ﺭﺋﻴﺲ', 'ﺍﻟﺤﻜﻮﻣﺔ', 'ﺍﻟﺤﻜﻮﻣﺔ', 'ﺭﺋﻴﺲ']\n",
        "\n",
        "text = \" \".join(test_tokens)\n",
        "# Apply Bag of Words\n",
        "vectorizer = CountVectorizer()\n",
        "bag_of_words = vectorizer.fit_transform([text]).toarray()\n",
        "\n",
        "print(\"\\nBag of Words:\")\n",
        "for token, count in zip(test_tokens, bag_of_words[0]):\n",
        "    print(f\"{token}: {count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbb8dafe-32c4-4fa8-861a-1d6ea8290ad1",
      "metadata": {
        "id": "fbb8dafe-32c4-4fa8-861a-1d6ea8290ad1"
      },
      "source": [
        "* apply on real data :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c6c6e9d-096a-421f-91ee-fbde91934664",
      "metadata": {
        "id": "8c6c6e9d-096a-421f-91ee-fbde91934664",
        "outputId": "c0d1e986-149a-4fb5-f5e8-88a87be9c945"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "display the encodage (Bag of Words) of the first '100' tokens : \n",
            "ﺿﻤﻦ: 1 || ﺍﻟﺘﻘﺮﻳﺮ: 1 || ﺗﺴﻠﻤﻪ: 1 || ﻋﺰﻳﺰ: 1 || ﺃﺧﻨﻮﺵ،: 1 || ﺭﺋﻴﺲ: 1 || ﺍﻟﺤﻜﻮﻣﺔ،: 2 || ﻧﻬﺎﻳﺔ: 1 || ﺷﻬﺮ: 1 || ﺍﻟﻤﺎﺿﻲ،: 1 || ﺍﻟﻬﻴﺌﺔ: 1 || ﺍﻟﻤﻜﻠﻔﺔ: 1 || ﺑﻤﺮﺍﺟﻌﺔ: 2 || ﻣﺪﻭﻧﺔ: 1 || ﺍﻷﺳﺮﺓ،: 2 || ﺍﻟﻤﻘﺮﺭ: 3 || ﺭﻓﻌﻪ: 1 || ﺍﻟﻤﻠﻚ: 1 || ﻣﺤﻤﺪ: 1 || ﺍﻟﺴﺎﺩﺱ،: 1 || ﻻﻓﺘﺎ: 1 || ﻭﺟﻮﺩ: 1 || ﻣﻘﺘﺮﺡ: 1 || ﺗﻄﺮﻕ: 2 || ﺗﻘﺴﻴﻢ: 1 || ﺍﻟﺘﺮﻛﺔ: 1 || ﺧﻼﻝ: 1 || ﺗﺨﻮﻳﻞ: 1 || ﺻﺎﺣﺐ: 1 || ﺍﻟﻤﺎﻝ: 1 || ﻭﺍﻷﺻﻮﻝ: 1 || ﺳﻠﻄﺔ: 1 || ﺍﺧﺘﻴﺎﺭ: 3 || ﺍﻟﻨﻈﺎﻡ: 1 || ﺍﻟﻤﻄﺒﻖ،: 1 || ﺍﻟﻤﻴﺮﺍﺙ: 1 || ﺍﻟﻮﺻﻴﺔ،: 1 || ﺗﻮﺳﻴﻊ: 1 || ﻧﻄﺎﻕ: 3 || ﺍﻷﺧﻴﺮﺓ: 1 || ﻟﺘﺸﻤﻞ: 1 || ﺍﻷﺣﻔﺎﺩ: 1 || ﻭﺭﻓﻊ: 1 || ﺍﻟﻘﻴﻮﺩ: 1 || ﺍﻟﻤﻔﺮﻭﺿﺔ: 1 || ﻋﻠﻴﻬﺎ: 9 || ﻭﻋﻠﻰ: 1 || ﺍﻟﺮﻏﻢ: 1 || ﺍﻟﻤﻘﺘﺮﺡ،: 1 || ﻋﺪﻩ: 1 || ﻛﺜﻴﺮﻭﻥ: 1 || ﺗﺴﺮﻳﺒﺎ،: 1 || ﻋﻤﻮﻣﺎ: 1 || ﻭﺛﻴﻘﺔ: 3 || ﺭﺍﺋﺠﺔ: 1 || ﻳﺘﺼﻞ: 1 || ﺑﺘﻮﺻﻴﺎﺕ: 1 || ﺭﺳﻤﻴﺔ: 2 || ﻣﺮﻓﻮﻋﺔ: 1 || ﺍﻟﻤﻠﻚ: 1 || ﻣﺤﻤﺪ: 1 || ﺍﻟﺴﺎﺩﺱ؛: 1 || ﺟﻬﺎﺕ: 1 || ﻛﺜﻴﺮﺓ: 1 || ﺍﻟﺘﻘﻄﺘﻪ،: 1 || ﺧﺼﻮﺻﺎ: 1 || ﺍﻷﻗﻠﻴﺎﺕ: 1 || ﺍﻟﺪﻳﻨﻴﺔ: 1 || ﺑﺎﻟﻤﻐﺮﺏ،: 1 || ﻭﻧﻈﺮﺕ: 1 || ﺇﻟﻴﻪ: 1 || ﺑـﺘﻘﺪﻳﺮ: 1 || ﻛـﺤﻞ: 1 || ﺑﺮﺍﻏﻤﺎﺗﻲ: 1 || ﻳﺴﺎﻋﺪ: 1 || ﺗﻘﺴﻴﻢ: 1 || ﺍﻟﺘﺮﻛﺎﺕ: 1 || ﻭﻓﻖ: 1 || ﻣﻨﻄﻖ: 2 || ﺍﻟﻮﺻﻴﺔ،: 1 || ﻭﺗﻤﻜﻴﻦ: 1 || ﺍﻟﻤﻐﺎﺭﺑﺔ: 1 || ﺍﺧﺘﻴﺎﺭ: 1 || ﻳﻨﺎﺳﺐ: 1 || ﺷﻜﻞ: 3 || ﻋﻴﺸﻬﻢ: 1 || ﺍﻟﺪﻳﻦ: 2 || ﻭﺍﻟﻘﺎﻧﻮﻥ: 1 || ﺳﻌﻴﺪ: 1 || ﻧﺎﺷﻴﺪ،: 2 || ﺣﻘﻮﻗﻲ: 1 || ﻭﺑﺎﺣﺚ: 2 || ﺍﻟﻔﻜﺮ: 1 || ﺍﻟﺪﻳﻨﻲ،: 2 || ﺍﻋﺘﺒﺮ: 3 || ﻣﻘﺘﺮﺡ: 1 || ﺗﻀﻤﻴﻦ: 1 || ﺍﻟﻮﺻﻴﺔ: 1 || ﺿﻤﻦ: 1 || ﺍﻻﺧﺘﻴﺎﺭﺍﺕ: 1 || "
          ]
        }
      ],
      "source": [
        "all_bag_words = []\n",
        "\n",
        "# tokenize each text\n",
        "for i in range(len(all_posts[\"tokens\"])):\n",
        "    # Convert tokens to a single string\n",
        "    text = \" \".join(all_posts[\"tokens\"][i])\n",
        "\n",
        "    # Apply Bag of Words\n",
        "    vectorizer = CountVectorizer()\n",
        "    bag_of_words = vectorizer.fit_transform([text]).toarray()\n",
        "\n",
        "    all_bag_words.append(bag_of_words)\n",
        "\n",
        "# display the Bag of Words of the first 100 tokens\n",
        "print(\"display the encodage (Bag of Words) of the first '100' tokens : \")\n",
        "for token, count in zip(all_posts[\"tokens\"][0][:100], bag_of_words[0][:100]):\n",
        "    print(f\"{token}: {count}\", end=\" || \")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e740e98d-2a52-4bf7-b1e7-2838590d23fc",
      "metadata": {
        "id": "e740e98d-2a52-4bf7-b1e7-2838590d23fc"
      },
      "source": [
        "#### 3. TF-IDF (Term Frequency-Inverse Document Frequency):"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3eb0652b-8a61-4367-a459-4248ac30b370",
      "metadata": {
        "id": "3eb0652b-8a61-4367-a459-4248ac30b370"
      },
      "source": [
        "* Test :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e2f3f4e-924a-4d93-a706-851586aa5ab2",
      "metadata": {
        "id": "6e2f3f4e-924a-4d93-a706-851586aa5ab2",
        "outputId": "4d3c89db-4e20-42d5-a3c0-ef377f186292"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "TF-IDF:\n",
            "ﺿﻤﻦ: 0.28\n",
            "ﺍﻟﺘﻘﺮﻳﺮ: 0.28\n",
            "ﺗﺴﻠﻤﻪ: 0.55\n",
            "ﻋﺰﻳﺰ: 0.28\n",
            "ﺃﺧﻨﻮﺵ: 0.55\n",
            "ﺭﺋﻴﺲ: 0.28\n",
            "ﺍﻟﺤﻜﻮﻣﺔ: 0.28\n"
          ]
        }
      ],
      "source": [
        "test_tokens = ['ﺿﻤﻦ', 'ﺍﻟﺘﻘﺮﻳﺮ', 'ﺗﺴﻠﻤﻪ', 'ﻋﺰﻳﺰ', 'ﺃﺧﻨﻮﺵ', 'ﺭﺋﻴﺲ', 'ﺍﻟﺤﻜﻮﻣﺔ', 'ﺍﻟﺤﻜﻮﻣﺔ', 'ﺭﺋﻴﺲ']\n",
        "text = \" \".join(test_tokens)\n",
        "# Apply TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf = tfidf_vectorizer.fit_transform([text]).toarray()\n",
        "\n",
        "print(\"\\nTF-IDF:\")\n",
        "for token, score in zip(test_tokens, tfidf[0]):\n",
        "    print(f\"{token}: {score:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e634b40-34e7-46a6-af9f-e35aaefc0316",
      "metadata": {
        "id": "0e634b40-34e7-46a6-af9f-e35aaefc0316"
      },
      "source": [
        "* apply on real data :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c4577ab-8302-4bec-af3d-7880024db9e4",
      "metadata": {
        "id": "0c4577ab-8302-4bec-af3d-7880024db9e4",
        "outputId": "2169bf22-9176-40a9-c4f3-280deb9410fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "display the score of TF-IDF of the first '100' tokens : \n",
            "ﺿﻤﻦ: 0.05 || ﺍﻟﺘﻘﺮﻳﺮ: 0.05 || ﺗﺴﻠﻤﻪ: 0.05 || ﻋﺰﻳﺰ: 0.05 || ﺃﺧﻨﻮﺵ،: 0.05 || ﺭﺋﻴﺲ: 0.05 || ﺍﻟﺤﻜﻮﻣﺔ،: 0.10 || ﻧﻬﺎﻳﺔ: 0.05 || ﺷﻬﺮ: 0.05 || ﺍﻟﻤﺎﺿﻲ،: 0.05 || ﺍﻟﻬﻴﺌﺔ: 0.05 || ﺍﻟﻤﻜﻠﻔﺔ: 0.05 || ﺑﻤﺮﺍﺟﻌﺔ: 0.10 || ﻣﺪﻭﻧﺔ: 0.05 || ﺍﻷﺳﺮﺓ،: 0.10 || ﺍﻟﻤﻘﺮﺭ: 0.15 || ﺭﻓﻌﻪ: 0.05 || ﺍﻟﻤﻠﻚ: 0.05 || ﻣﺤﻤﺪ: 0.05 || ﺍﻟﺴﺎﺩﺱ،: 0.05 || ﻻﻓﺘﺎ: 0.05 || ﻭﺟﻮﺩ: 0.05 || ﻣﻘﺘﺮﺡ: 0.05 || ﺗﻄﺮﻕ: 0.10 || ﺗﻘﺴﻴﻢ: 0.05 || ﺍﻟﺘﺮﻛﺔ: 0.05 || ﺧﻼﻝ: 0.05 || ﺗﺨﻮﻳﻞ: 0.05 || ﺻﺎﺣﺐ: 0.05 || ﺍﻟﻤﺎﻝ: 0.05 || ﻭﺍﻷﺻﻮﻝ: 0.05 || ﺳﻠﻄﺔ: 0.05 || ﺍﺧﺘﻴﺎﺭ: 0.15 || ﺍﻟﻨﻈﺎﻡ: 0.05 || ﺍﻟﻤﻄﺒﻖ،: 0.05 || ﺍﻟﻤﻴﺮﺍﺙ: 0.05 || ﺍﻟﻮﺻﻴﺔ،: 0.05 || ﺗﻮﺳﻴﻊ: 0.05 || ﻧﻄﺎﻕ: 0.15 || ﺍﻷﺧﻴﺮﺓ: 0.05 || ﻟﺘﺸﻤﻞ: 0.05 || ﺍﻷﺣﻔﺎﺩ: 0.05 || ﻭﺭﻓﻊ: 0.05 || ﺍﻟﻘﻴﻮﺩ: 0.05 || ﺍﻟﻤﻔﺮﻭﺿﺔ: 0.05 || ﻋﻠﻴﻬﺎ: 0.45 || ﻭﻋﻠﻰ: 0.05 || ﺍﻟﺮﻏﻢ: 0.05 || ﺍﻟﻤﻘﺘﺮﺡ،: 0.05 || ﻋﺪﻩ: 0.05 || ﻛﺜﻴﺮﻭﻥ: 0.05 || ﺗﺴﺮﻳﺒﺎ،: 0.05 || ﻋﻤﻮﻣﺎ: 0.05 || ﻭﺛﻴﻘﺔ: 0.15 || ﺭﺍﺋﺠﺔ: 0.05 || ﻳﺘﺼﻞ: 0.05 || ﺑﺘﻮﺻﻴﺎﺕ: 0.05 || ﺭﺳﻤﻴﺔ: 0.10 || ﻣﺮﻓﻮﻋﺔ: 0.05 || ﺍﻟﻤﻠﻚ: 0.05 || ﻣﺤﻤﺪ: 0.05 || ﺍﻟﺴﺎﺩﺱ؛: 0.05 || ﺟﻬﺎﺕ: 0.05 || ﻛﺜﻴﺮﺓ: 0.05 || ﺍﻟﺘﻘﻄﺘﻪ،: 0.05 || ﺧﺼﻮﺻﺎ: 0.05 || ﺍﻷﻗﻠﻴﺎﺕ: 0.05 || ﺍﻟﺪﻳﻨﻴﺔ: 0.05 || ﺑﺎﻟﻤﻐﺮﺏ،: 0.05 || ﻭﻧﻈﺮﺕ: 0.05 || ﺇﻟﻴﻪ: 0.05 || ﺑـﺘﻘﺪﻳﺮ: 0.05 || ﻛـﺤﻞ: 0.05 || ﺑﺮﺍﻏﻤﺎﺗﻲ: 0.05 || ﻳﺴﺎﻋﺪ: 0.05 || ﺗﻘﺴﻴﻢ: 0.05 || ﺍﻟﺘﺮﻛﺎﺕ: 0.05 || ﻭﻓﻖ: 0.05 || ﻣﻨﻄﻖ: 0.10 || ﺍﻟﻮﺻﻴﺔ،: 0.05 || ﻭﺗﻤﻜﻴﻦ: 0.05 || ﺍﻟﻤﻐﺎﺭﺑﺔ: 0.05 || ﺍﺧﺘﻴﺎﺭ: 0.05 || ﻳﻨﺎﺳﺐ: 0.05 || ﺷﻜﻞ: 0.15 || ﻋﻴﺸﻬﻢ: 0.05 || ﺍﻟﺪﻳﻦ: 0.10 || ﻭﺍﻟﻘﺎﻧﻮﻥ: 0.05 || ﺳﻌﻴﺪ: 0.05 || ﻧﺎﺷﻴﺪ،: 0.10 || ﺣﻘﻮﻗﻲ: 0.05 || ﻭﺑﺎﺣﺚ: 0.10 || ﺍﻟﻔﻜﺮ: 0.05 || ﺍﻟﺪﻳﻨﻲ،: 0.10 || ﺍﻋﺘﺒﺮ: 0.15 || ﻣﻘﺘﺮﺡ: 0.05 || ﺗﻀﻤﻴﻦ: 0.05 || ﺍﻟﻮﺻﻴﺔ: 0.05 || ﺿﻤﻦ: 0.05 || ﺍﻻﺧﺘﻴﺎﺭﺍﺕ: 0.05 || "
          ]
        }
      ],
      "source": [
        "all_tf_idf = []\n",
        "\n",
        "for i in range(len(all_posts[\"tokens\"])):\n",
        "    # Convert tokens to a single string\n",
        "    text = \" \".join(all_posts[\"tokens\"][i])\n",
        "    # Apply TF-IDF\n",
        "    tfidf_vectorizer = TfidfVectorizer()\n",
        "    tfidf = tfidf_vectorizer.fit_transform([text]).toarray()\n",
        "    all_tf_idf.append(tfidf)\n",
        "\n",
        "# display the score of TF-IDF of the first 100 tokens\n",
        "print(\"display the score of TF-IDF of the first '100' tokens : \")\n",
        "for token, score in zip(all_posts[\"tokens\"][0][:100], tfidf[0][:100]):\n",
        "    print(f\"{token}: {score:.2f}\", end=\" || \")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbbf2d53-ea52-4b97-a25b-f12a162c233e",
      "metadata": {
        "id": "cbbf2d53-ea52-4b97-a25b-f12a162c233e"
      },
      "source": [
        "### 2. Apply one Word2Vec Approach (Skip Gram, CBOW) on the same DataSet :"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sPLLGrOwiOkH",
      "metadata": {
        "id": "sPLLGrOwiOkH"
      },
      "source": [
        "* Example / test :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "e46dce1f-6f44-40cc-a6f1-7d1cda0f3c79",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e46dce1f-6f44-40cc-a6f1-7d1cda0f3c79",
        "outputId": "f4972aa6-cfc5-4793-9f1e-9a34c591245b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word vector: [ 8.1681199e-03 -4.4430327e-03  8.9854337e-03  8.2536647e-03\n",
            " -4.4352221e-03  3.0310510e-04  4.2744912e-03 -3.9263200e-03\n",
            " -5.5599655e-03 -6.5123225e-03 -6.7073823e-04 -2.9592158e-04\n",
            "  4.4630850e-03 -2.4740540e-03 -1.7260908e-04  2.4618758e-03\n",
            "  4.8675989e-03 -3.0808449e-05 -6.3394094e-03 -9.2608072e-03\n",
            "  2.6657581e-05  6.6618943e-03  1.4660227e-03 -8.9665223e-03\n",
            " -7.9386048e-03  6.5519023e-03 -3.7856805e-03  6.2549924e-03\n",
            " -6.6810320e-03  8.4796622e-03 -6.5163244e-03  3.2880199e-03\n",
            " -1.0569858e-03 -6.7875278e-03 -3.2875966e-03 -1.1614120e-03\n",
            " -5.4709399e-03 -1.2113475e-03 -7.5633135e-03  2.6466595e-03\n",
            "  9.0701487e-03 -2.3772502e-03 -9.7651005e-04  3.5135616e-03\n",
            "  8.6650876e-03 -5.9218528e-03 -6.8875779e-03 -2.9329848e-03\n",
            "  9.1476962e-03  8.6626766e-04 -8.6784009e-03 -1.4469790e-03\n",
            "  9.4794659e-03 -7.5494875e-03 -5.3580985e-03  9.3165627e-03\n",
            " -8.9737261e-03  3.8259076e-03  6.6544057e-04  6.6607012e-03\n",
            "  8.3127534e-03 -2.8507852e-03 -3.9923131e-03  8.8979173e-03\n",
            "  2.0896459e-03  6.2489416e-03 -9.4457148e-03  9.5901238e-03\n",
            " -1.3483083e-03 -6.0521150e-03  2.9925345e-03 -4.5661093e-04\n",
            "  4.7064926e-03 -2.2830211e-03 -4.1378425e-03  2.2778988e-03\n",
            "  8.3543835e-03 -4.9956059e-03  2.6686788e-03 -7.9905549e-03\n",
            " -6.7733466e-03 -4.6766878e-04 -8.7677278e-03  2.7894378e-03\n",
            "  1.5985954e-03 -2.3196924e-03  5.0037908e-03  9.7487867e-03\n",
            "  8.4542679e-03 -1.8802249e-03  2.0581519e-03 -4.0036892e-03\n",
            " -8.2414057e-03  6.2779556e-03 -1.9491815e-03 -6.6620467e-04\n",
            " -1.7713320e-03 -4.5356657e-03  4.0617096e-03 -4.2701806e-03]\n",
            "Similar words: [('في', 0.09291724115610123), ('الطقس', 0.00484249135479331), ('بارداً', -0.0027540253940969706), ('و', -0.013679751195013523), ('الصيف', -0.028491031378507614), ('حاراً', -0.05774581804871559), ('الشتاء', -0.11555545777082443)]\n",
            "**************************************************\n",
            "Word vector (CBOW): [ 8.1681199e-03 -4.4430327e-03  8.9854337e-03  8.2536647e-03\n",
            " -4.4352221e-03  3.0310510e-04  4.2744912e-03 -3.9263200e-03\n",
            " -5.5599655e-03 -6.5123225e-03 -6.7073823e-04 -2.9592158e-04\n",
            "  4.4630850e-03 -2.4740540e-03 -1.7260908e-04  2.4618758e-03\n",
            "  4.8675989e-03 -3.0808449e-05 -6.3394094e-03 -9.2608072e-03\n",
            "  2.6657581e-05  6.6618943e-03  1.4660227e-03 -8.9665223e-03\n",
            " -7.9386048e-03  6.5519023e-03 -3.7856805e-03  6.2549924e-03\n",
            " -6.6810320e-03  8.4796622e-03 -6.5163244e-03  3.2880199e-03\n",
            " -1.0569858e-03 -6.7875278e-03 -3.2875966e-03 -1.1614120e-03\n",
            " -5.4709399e-03 -1.2113475e-03 -7.5633135e-03  2.6466595e-03\n",
            "  9.0701487e-03 -2.3772502e-03 -9.7651005e-04  3.5135616e-03\n",
            "  8.6650876e-03 -5.9218528e-03 -6.8875779e-03 -2.9329848e-03\n",
            "  9.1476962e-03  8.6626766e-04 -8.6784009e-03 -1.4469790e-03\n",
            "  9.4794659e-03 -7.5494875e-03 -5.3580985e-03  9.3165627e-03\n",
            " -8.9737261e-03  3.8259076e-03  6.6544057e-04  6.6607012e-03\n",
            "  8.3127534e-03 -2.8507852e-03 -3.9923131e-03  8.8979173e-03\n",
            "  2.0896459e-03  6.2489416e-03 -9.4457148e-03  9.5901238e-03\n",
            " -1.3483083e-03 -6.0521150e-03  2.9925345e-03 -4.5661093e-04\n",
            "  4.7064926e-03 -2.2830211e-03 -4.1378425e-03  2.2778988e-03\n",
            "  8.3543835e-03 -4.9956059e-03  2.6686788e-03 -7.9905549e-03\n",
            " -6.7733466e-03 -4.6766878e-04 -8.7677278e-03  2.7894378e-03\n",
            "  1.5985954e-03 -2.3196924e-03  5.0037908e-03  9.7487867e-03\n",
            "  8.4542679e-03 -1.8802249e-03  2.0581519e-03 -4.0036892e-03\n",
            " -8.2414057e-03  6.2779556e-03 -1.9491815e-03 -6.6620467e-04\n",
            " -1.7713320e-03 -4.5356657e-03  4.0617096e-03 -4.2701806e-03]\n",
            "Similar words (CBOW): [('في', 0.09291724115610123), ('الطقس', 0.00484249135479331), ('بارداً', -0.0027540253940969706), ('و', -0.013679751195013523), ('الصيف', -0.028491031378507614), ('حاراً', -0.05774581804871559), ('الشتاء', -0.11555545777082443)]\n"
          ]
        }
      ],
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Example Arabic tokens\n",
        "arabic_tokens = [[\"كان\", \"الطقس\", \"حاراً\", \"في\", \"الصيف\", \"و\", \"بارداً\", \"في\", \"الشتاء\"]]\n",
        "\n",
        "# Train Word2Vec model (Skip-gram)\n",
        "model_skipgram_test = Word2Vec(sentences=arabic_tokens, vector_size=100, window=5, sg=1, min_count=1)\n",
        "\n",
        "# Train Word2Vec model (CBOW)\n",
        "model_cbow_test = Word2Vec(sentences=arabic_tokens, vector_size=100, window=5, sg=0, min_count=1)\n",
        "\n",
        "# Get vector representation of a word\n",
        "word_vector1 = model_skipgram_test.wv.get_vector(\"كان\")\n",
        "\n",
        "# Find similar words\n",
        "similar_words1 = model_skipgram_test.wv.most_similar(\"كان\")\n",
        "\n",
        "print(\"Word vector:\", word_vector1)\n",
        "print(\"Similar words:\", similar_words1)\n",
        "\n",
        "print(\"*\"*50)\n",
        "\n",
        "# Get vector representation of a word using CBOW model\n",
        "word_vector_cbow1 = model_cbow_test.wv.get_vector(\"كان\")\n",
        "\n",
        "# Find similar words using CBOW model\n",
        "similar_words_cbow1 = model_cbow_test.wv.most_similar(\"كان\")\n",
        "\n",
        "print(\"Word vector (CBOW):\", word_vector_cbow1)\n",
        "print(\"Similar words (CBOW):\", similar_words_cbow1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vHWjTL57cRAR",
      "metadata": {
        "id": "vHWjTL57cRAR"
      },
      "source": [
        "* apply on real data :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "9d05101f-69ca-4111-a9bc-fbf4b137c292",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9d05101f-69ca-4111-a9bc-fbf4b137c292",
        "outputId": "4b5caa83-64ac-4107-cf56-12a87a571809"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tokens :  [['ﺿﻤﻦ', 'ﺍﻟﺘﻘﺮﻳﺮ', 'ﺗﺴﻠﻤﻪ', 'ﻋﺰﻳﺰ', 'ﺃﺧﻨﻮﺵ،', 'ﺭﺋﻴﺲ', 'ﺍﻟﺤﻜﻮﻣﺔ،', 'ﻧﻬﺎﻳﺔ', 'ﺷﻬﺮ', 'ﺍﻟﻤﺎﺿﻲ،', 'ﺍﻟﻬﻴﺌﺔ', 'ﺍﻟﻤﻜﻠﻔﺔ', 'ﺑﻤﺮﺍﺟﻌﺔ', 'ﻣﺪﻭﻧﺔ', 'ﺍﻷﺳﺮﺓ،', 'ﺍﻟﻤﻘﺮﺭ', 'ﺭﻓﻌﻪ', 'ﺍﻟﻤﻠﻚ', 'ﻣﺤﻤﺪ', 'ﺍﻟﺴﺎﺩﺱ،', 'ﻻﻓﺘﺎ', 'ﻭﺟﻮﺩ', 'ﻣﻘﺘﺮﺡ', 'ﺗﻄﺮﻕ', 'ﺗﻘﺴﻴﻢ', 'ﺍﻟﺘﺮﻛﺔ', 'ﺧﻼﻝ', 'ﺗﺨﻮﻳﻞ', 'ﺻﺎﺣﺐ', 'ﺍﻟﻤﺎﻝ', 'ﻭﺍﻷﺻﻮﻝ', 'ﺳﻠﻄﺔ', 'ﺍﺧﺘﻴﺎﺭ', 'ﺍﻟﻨﻈﺎﻡ', 'ﺍﻟﻤﻄﺒﻖ،', 'ﺍﻟﻤﻴﺮﺍﺙ', 'ﺍﻟﻮﺻﻴﺔ،', 'ﺗﻮﺳﻴﻊ', 'ﻧﻄﺎﻕ', 'ﺍﻷﺧﻴﺮﺓ', 'ﻟﺘﺸﻤﻞ', 'ﺍﻷﺣﻔﺎﺩ', 'ﻭﺭﻓﻊ', 'ﺍﻟﻘﻴﻮﺩ', 'ﺍﻟﻤﻔﺮﻭﺿﺔ', 'ﻋﻠﻴﻬﺎ', 'ﻭﻋﻠﻰ', 'ﺍﻟﺮﻏﻢ', 'ﺍﻟﻤﻘﺘﺮﺡ،', 'ﻋﺪﻩ', 'ﻛﺜﻴﺮﻭﻥ', 'ﺗﺴﺮﻳﺒﺎ،', 'ﻋﻤﻮﻣﺎ', 'ﻭﺛﻴﻘﺔ', 'ﺭﺍﺋﺠﺔ', 'ﻳﺘﺼﻞ', 'ﺑﺘﻮﺻﻴﺎﺕ', 'ﺭﺳﻤﻴﺔ', 'ﻣﺮﻓﻮﻋﺔ', 'ﺍﻟﻤﻠﻚ', 'ﻣﺤﻤﺪ', 'ﺍﻟﺴﺎﺩﺱ؛', 'ﺟﻬﺎﺕ', 'ﻛﺜﻴﺮﺓ', 'ﺍﻟﺘﻘﻄﺘﻪ،', 'ﺧﺼﻮﺻﺎ', 'ﺍﻷﻗﻠﻴﺎﺕ', 'ﺍﻟﺪﻳﻨﻴﺔ', 'ﺑﺎﻟﻤﻐﺮﺏ،', 'ﻭﻧﻈﺮﺕ', 'ﺇﻟﻴﻪ', 'ﺑـﺘﻘﺪﻳﺮ', 'ﻛـﺤﻞ', 'ﺑﺮﺍﻏﻤﺎﺗﻲ', 'ﻳﺴﺎﻋﺪ', 'ﺗﻘﺴﻴﻢ', 'ﺍﻟﺘﺮﻛﺎﺕ', 'ﻭﻓﻖ', 'ﻣﻨﻄﻖ', 'ﺍﻟﻮﺻﻴﺔ،', 'ﻭﺗﻤﻜﻴﻦ', 'ﺍﻟﻤﻐﺎﺭﺑﺔ', 'ﺍﺧﺘﻴﺎﺭ', 'ﻳﻨﺎﺳﺐ', 'ﺷﻜﻞ', 'ﻋﻴﺸﻬﻢ', 'ﺍﻟﺪﻳﻦ', 'ﻭﺍﻟﻘﺎﻧﻮﻥ', 'ﺳﻌﻴﺪ', 'ﻧﺎﺷﻴﺪ،', 'ﺣﻘﻮﻗﻲ', 'ﻭﺑﺎﺣﺚ', 'ﺍﻟﻔﻜﺮ', 'ﺍﻟﺪﻳﻨﻲ،', 'ﺍﻋﺘﺒﺮ', 'ﻣﻘﺘﺮﺡ', 'ﺗﻀﻤﻴﻦ', 'ﺍﻟﻮﺻﻴﺔ', 'ﺿﻤﻦ', 'ﺍﻻﺧﺘﻴﺎﺭﺍﺕ', 'ﺗﻮﺯﻳﻊ', 'ﺍﻟﺘﺮﻛﺔ', 'ﻳﻜﺸﻒ', 'ﻓﻌﻼ', 'ﺣﻞ', 'ﺑﺮﺍﻏﻤﺎﺗﻲ', 'ﻳﺴﺎﻫﻢ', 'ﺍﻟﺘﺨﻠﺺ', 'ﻣﺄﺯﻕ', 'ﻓﺮﺽ', 'ﺗﻘﺴﻴﻢ', 'ﺍﻟﺘﺮﻛﺔ', 'ﻭﻓﻖ', 'ﺍﻟﻤﻨﻈﻮﺭ', 'ﺍﻹﺳﻼﻣﻲ', 'ﺃﻣﻼﻙ', 'ﺍﻟﻤﺴﻠﻤﻴﻦ،', 'ﺳﻮﺍﺀ', 'ﻛﺎﻧﻮﺍ', 'ﻣﺴﻴﺤﻴﻴﻦ', 'ﺑﻬﺎﺋﻴﻴﻦ', 'ﻣﻼﺣﺪﺓ،', 'ﻣﺆﻛﺪﺍ', 'ﺍﻻﺷﺘﻐﺎﻝ', 'ﺑﺎﻟﻮﺻﻴﺔ', 'ﺳﻴﻜﻮﻥ', 'ﻣﺒﺪﺃ', 'ﻗﺎﻧﻮﻧﻴﺎ', 'ﺻﺮﻓﺎ،', 'ﻛﺎﻧﺖ', 'ﻭﺍﺭﺩﺓ', 'ﺍﻟﻨﺺ', 'ﺍﻟﺪﻳﻨﻲ', 'ﺍﻟﻘﺮﺁﻧﻲ', 'ﻭﻭﺿﺢ', 'ﻧﺎﺷﻴﺪ،', 'ﺗﺼﺮﻳﺢ', 'ﻟﻬﺴﺒﺮﻳﺲ،', 'ﺍﻟﻮﺻﻴﺔ', 'ﺗﻤﺜﻞ', 'ﻣﺨﺮﺟﺎ', 'ﻭﺣﻼ', 'ﻟﻤﺸﻜﻞ', 'ﺍﻹﺭﺙ', 'ﺩﺍﺧﻞ', 'ﺑﻨﻴﺔ', 'ﺍﻟﺪﻳﻦ', 'ﻧﻔﺴﻪ،', 'ﻭﻫﻲ', 'ﺩﻟﻴﻞ', 'ﺍﻟﺪﻳﻦ', 'ﻭﺍﺳﻊ', 'ﻭﺭﺣﺐ', 'ﻳﻜﻔﻲ', 'ﻟﻨﺠﺪ', 'ﻣﺨﺎﺭﺝ', 'ﻟﻜﻞ', 'ﺍﻟﻤﺸﺎﻛﻞ', 'ﺍﻟﻤﻄﺮﻭﺣﺔ', 'ﺗﺪﺑﻴﺮ', 'ﺍﻟﻤﺠﺘﻤﻊ،', 'ﻣﻌﺘﺒﺮﺍ', 'ﺇﺻﺮﺍﺭ', 'ﺍﻟﺠﻬﺔ', 'ﺍﻟﻤﺤﺎﻓﻈﺔ', 'ﺗﻀﻴﻴﻖ', 'ﺍﻟﺨﻨﺎﻕ', 'ﻳﻤﺖ', 'ﺟﻮﻫﺮ', 'ﺍﻟﺪﻳﻦ', 'ﺑﺄﻳﺔ', 'ﺻﻠﺔ؛', 'ﻷﻥ', 'ﻣﻨﻈﻮﻣﺔ', 'ﺍﻹﺭﺙ', 'ﺳﺒﻖ', 'ﻟﺘﻘﺴﻴﻤﺎﺗﻬﺎ', 'ﻭﻗﻌﺖ', 'ﺗﻌﺪﻳﻼﺕ', 'ﻛﺜﻴﺮﺓ،', 'ﻟﺘﻌﻄﻴﻨﺎ', 'ﻣﺨﺎﺭﺝ', 'ﻧﻮﺍﺯﻝ', 'ﻭﻣﺴﺘﺠﺪﺍﺕ', 'ﻭﻟﻔﺖ', 'ﺍﻟﻤﺘﺤﺪﺙ', 'ﻋﻴﻨﻪ', 'ﺃﻫﻤﻴﺔ', 'ﺍﻟﻮﺻﻴﺔ', 'ﺑﺎﻟﻨﺴﺒﺔ', 'ﻟﻐﻴﺮ', 'ﺍﻟﻤﺴﻠﻤﻴﻦ،', 'ﻟﻜﻮﻧﻬﺎ', 'ﻣﺪﺧﻼ', 'ﻻﺧﺘﻴﺎﺭ', 'ﺍﻟﺸﻜﻞ', 'ﺍﻟﻤﻨﺎﺳﺐ', 'ﻟﻠﺘﻌﺎﻃﻲ', 'ﺗﺮﻛﺎﺗﻬﻢ،', 'ﻭﺣﺪﻫﻢ', 'ﺍﻟﺤﻖ', 'ﻓﻴﻬﺎ،', 'ﻣﺴﺠﻼ', 'ﺻﻌﻮﺑﺔ', 'ﺍﻟﺪﺧﻮﻝ', 'ﺟﺰﺋﻴﺎﺕ', 'ﻛﺜﻴﺮﺓ', 'ﺗﻌﻮﺩ', 'ﺩﻳﺎﻧﺔ', 'ﻭﺃﻧﻤﺎﻃﻬﺎ', 'ﺗﻘﺴﻴﻢ', 'ﺍﻹﺭﺙ؛', 'ﻋﻤﻠﻴﺎ', 'ﺍﻟﻮﺻﻴﺔ', 'ﺫﻛﺎﺀ', 'ﻗﺎﻧﻮﻧﻲ', 'ﻳﻨﺴﺠﻢ', 'ﻣﺒﺎﺩﺉ', 'ﺍﻟﺪﻭﻟﺔ،', 'ﻭﻳﺤﺘﺮﻡ', 'ﺍﻟﺨﻴﺎﺭﺍﺕ', 'ﺍﻟﺮﻭﺣﻴﺔ', 'ﺗﻌﻴﺶ', 'ﺩﺍﺧﻞ', 'ﺣﺪﻭﺩ', 'ﺍﻟﺪﻭﻟﺔ', 'ﺍﻟﻮﻃﻨﻴﺔ', 'ﺗﺪﺑﻴﺮ', 'ﺩﻳﻤﻘﺮﺍﻃﻲ', 'ﺁﺩﻡ', 'ﺍﻟﺮﺑﺎﻃﻲ،', 'ﺭﺋﻴﺲ', 'ﺍﺗﺤﺎﺩ', 'ﺍﻟﻤﺴﻴﺤﻴﻴﻦ', 'ﺍﻟﻤﻐﺎﺭﺑﺔ،', 'ﺗﻤﻨﻰ', 'ﻳﺘﻢ', 'ﺇﺩﺭﺍﺝ', 'ﺍﻟﻮﺻﻴﺔ', 'ﻣﺪﻭﻧﺔ', 'ﺍﻷﺳﺮﺓ', 'ﺍﻟﺠﺪﻳﺪﺓ،', 'ﻛـﺈﻧﺼﺎﻑ', 'ﻷﻗﻠﻴﺎﺕ', 'ﺩﻳﻨﻴﺔ', 'ﻣﻮﺟﻮﺩﺓ', 'ﻭﺑﻘﻮﺓ', 'ﺍﻟﻤﺠﺘﻤﻊ', 'ﺍﻟﻤﻐﺮﺑﻲ،', 'ﻣﺸﻴﺮﺍ', 'ﺍﻟﻮﺻﻴﺔ', 'ﻣﻌﻤﻮﻝ', 'ﺍﻟﻌﺪﻳﺪ', 'ﺍﻟﺪﻭﻝ', 'ﺣﻮﻝ', 'ﺍﻟﻌﺎﻟﻢ؛', 'ﻭﺑﺎﻟﺘﺎﻟﻲ', 'ﻳﻤﻜﻨﻨﺎ', 'ﻧﺮﺣﺐ', 'ﻭﻧﺪﻋﻤﻬﺎ،', 'ﻷﻧﻬﺎ', 'ﺳﺘﻔﺘﺢ', 'ﺃﺑﻮﺍﺑﺎ', 'ﺩﻳﻤﻘﺮﺍﻃﻴﺔ', 'ﺗﻨﻈﻴﻢ', 'ﺍﻷﺳﺮﺓ', 'ﺍﻟﻤﺴﻴﺤﻴﺔ', 'ﺍﻟﺒﻬﺎﺋﻴﺔ', 'ﻏﻴﺮﻫﻤﺎ،', 'ﻟﻜﻮﻧﻨﺎ', 'ﺳﻨﺘﻌﺎﻣﻞ', 'ﻣﻌﻬﺎ', 'ﻛﻘﺎﻧﻮﻥ', 'ﻣﺠﺮﺩ', 'ﻭﻟﻴﺲ', 'ﺷﻴﺌﺎ', 'ﺁﺧﺮ', 'ﻭﺷﺪﺩ', 'ﺍﻟﺮﺑﺎﻃﻲ،', 'ﺿﻤﻦ', 'ﺗﺼﺮﻳﺤﻪ', 'ﻟﻬﺴﺒﺮﻳﺲ،', 'ﺇﺩﺭﺍﺝ', 'ﺍﻟﻮﺻﻴﺔ', 'ﺣﻘﻴﻘﻴﺎ،', 'ﻓﻬﺬﺍ', 'ﺳﻴﻜﻮﻥ', 'ﻓﺘﺤﺎ', 'ﻟﻠﺒﺎﺏ', 'ﺍﺟﺘﻬﺎﺩﺍﺕ', 'ﺃﺧﺮﻯ', 'ﻣﺴﺘﻘﺒﻼ،', 'ﺗﺴﺘﺜﻤﺮ', 'ﻳﺴﻤﻰ', 'ﺑﺪﻳﻦ', 'ﺍﻟﺪﻭﻟﺔ؛', 'ﺍﺣﺘﺮﺍﻡ', 'ﻟﻠﻌﻘﺎﺋﺪ', 'ﺍﻷﺧﺮﻯ', 'ﺍﻟﻤﻮﺟﻮﺩﺓ،', 'ﺛﻘﺔ', 'ﺃﻣﻴﺮ', 'ﺍﻟﻤﺆﻣﻨﻴﻦ،', 'ﺃﻛﺪ', 'ﺇﻣﺎﺭﺓ', 'ﺍﻟﻤﺆﻣﻨﻴﻦ', 'ﺗﻌﻨﻲ', 'ﺍﻷﺩﻳﺎﻥ', 'ﻭﻛﻞ', 'ﺍﻟﻘﻨﺎﻋﺎﺕ', 'ﺍﻟﺮﻭﺣﻴﺔ', 'ﻭﺍﻷﺧﻼﻗﻴﺔ', 'ﻛﺎﻧﺖ', 'ﺩﺭﺟﺔ', 'ﺣﻀﻮﺭﻫﺎ', 'ﺍﻟﻔﻀﺎﺀ', 'ﺍﻟﻌﺎﻡ', 'ﺍﻟﻤﻐﺮﺑﻲ', 'ﻭﺍﻋﺘﺒﺮ', 'ﺍﻟﻤﺘﺤﺪﺙ', 'ﻋﻴﻨﻪ', 'ﺍﻟﻮﺻﻴﺔ', 'ﺗﺴﺎﻫﻢ', 'ﺗﺤﻘﻴﻖ', 'ﺍﻟﺤﺪ', 'ﺍﻷﺩﻧﻰ', 'ﺑﺎﻟﻨﺴﺒﺔ', 'ﻟﻠﻌﺎﺋﻼﺕ', 'ﺍﻟﻤﺴﻠﻤﺔ،', 'ﺃﻧﻨﺎ', 'ﻧﺸﺪﺩ', 'ﺍﻹﺻﻼﺣﺎﺕ', 'ﻭﺭﺩﺕ', 'ﺍﻟﺘﻮﺻﻴﺎﺕ', 'ﺍﻟﻤﺘﺪﺍﻭﻟﺔ،', 'ﺗﻜﻦ', 'ﺭﺳﻤﻴﺔ،', 'ﻓﻬﻲ', 'ﺗﻜﻦ', 'ﻣﺪﻧﻴﺔ', 'ﺍﻟﻤﺎﺋﺔ؛', 'ﻟﻜﻨﻬﺎ', 'ﻛﺎﻧﺖ', 'ﺫﻛﻴﺔ', 'ﻓﺘﺢ', 'ﺍﻟﺒﺎﺏ', 'ﺍﻟﺘﻄﻮﻳﺮ', 'ﺍﻟﻤﺴﺘﻘﺒﻞ،', 'ﻭﺭﻓﻊ', 'ﺍﻟﺤﺮﺝ', 'ﻳﻌﺘﺒﺮﻭﻥ', 'ﺃﺩﻳﺎﻧﺎ', 'ﺃﺧﺮﻯ', 'ﺗﻮﻗﻴﻌﺎ', 'ﺭﻭﺣﻴﺎ', 'ﻳﺨﻮﻝ', 'ﺍﻻﻧﺘﻤﺎﺀ', 'ﻧﺎﺩﻱ', 'ﺍﻹﻧﺴﺎﻧﻴﺔ', 'ﺭﻓﺾ', 'ﺍﻟﺘﺤﺎﻳﻞ', 'ﻋﺒﺪ', 'ﺍﻹﻟﻪ', 'ﺍﻟﺨﻀﺮﻱ،', 'ﺭﺋﻴﺲ', 'ﺍﻟﻤﺮﻛﺰ', 'ﺍﻟﻤﻐﺮﺑﻲ', 'ﻟﺤﻘﻮﻕ', 'ﺍﻹﻧﺴﺎﻥ،', 'ﻗﺎﻝ', 'ﺍﻟﻮﺻﻴﺔ', 'ﺑﺎﻟﺸﻜﻞ', 'ﺗﻢ', 'ﺗﺼﻮﻳﺮﻫﺎ', 'ﻣﺮﻓﻮﺿﺔ', 'ﺗﻤﺎﻣﺎ،', 'ﻭﻫﻲ', 'ﻣﺤﺎﻭﻟﺔ', 'ﻟﻠﺘﺤﺎﻳﻞ', 'ﻗﻮﺍﻋﺪ', 'ﺍﻹﺭﺙ', 'ﻭﺍﺳﺘﻌﻤﺎﻝ', 'ﺧﺎﻃﺊ', 'ﻟﻤﻔﻬﻮﻡ', 'ﺍﻟﻮﺻﻴﺔ،', 'ﻣﻌﺘﺒﺮﺍ', 'ﺍﻹﺭﺙ', 'ﻋﻘﺪﺍ', 'ﻋﻮﺿﻴﺎ', 'ﺗﺒﺮﻋﻴﺎ', 'ﺍﻟﺒﺸﺮ،', 'ﻭﻣﻦ', 'ﺛﻤﺔ', 'ﻳﺠﺮﻱ', 'ﻣﺠﺮﻯ', 'ﺍﻟﻌﻘﺪ؛', 'ﺍﻟﻮﺻﻴﺔ', 'ﺗﻌﺘﺒﺮ', 'ﻋﻘﺪﺍ', 'ﻣﺴﻤﻰ', 'ﺍﻟﻌﻘﻮﺩ', 'ﺍﻟﺘﺒﺮﻋﻴﺔ،', 'ﺗﺆﻃﺮﻫﺎ', 'ﻧﺼﻮﺹ', 'ﺗﺸﺮﻳﻌﻴﺔ', 'ﻭﺗﺘﺮﺗﺐ', 'ﻋﻨﻬﺎ', 'ﺃﺣﻜﺎﻡ', 'ﻭﺃﻭﺿﺢ', 'ﺍﻟﺨﻀﺮﻱ،', 'ﺗﺼﺮﻳﺢ', 'ﻟﻬﺴﺒﺮﻳﺲ،', 'ﺃﻧﻪ', 'ﺣﺮﻳﺎ', 'ﺍﻗﺘﺮﺣﻮﺍ', 'ﺍﻟﻤﻘﺎﺭﺑﺔ', 'ﻳﻌﻮﺿﻮﺍ', 'ﺍﻟﻮﺻﻴﺔ', 'ﺍﻟﻬﺒﺔ؛', 'ﻓﺎﻟﻬﺒﺔ', 'ﻋﻘﺪ', 'ﻣﺴﻤﻰ', 'ﺗﺒﺮﻋﻲ', 'ﻣﺜﻠﻪ', 'ﺍﻟﻮﺻﻴﺔ،', 'ﺍﻟﻬﺒﺔ', 'ﺗﺒﺮﻉ', 'ﻭﺗﻨﻔﻴﺬ', 'ﺣﺎﻝ', 'ﺍﻟﺤﻴﺎﺓ،', 'ﺍﻟﻮﺻﻴﺔ', 'ﺗﺒﺮﻉ', 'ﺣﺎﻝ', 'ﺍﻟﺤﻴﺎﺓ', 'ﻭﺗﻨﻔﻴﺬ', 'ﺍﻟﻤﻮﺕ،', 'ﺳﺎﺋﺮ', 'ﺍﻟﻌﻘﻮﺩ', 'ﺍﻟﻤﺴﻤﺎﺓ', 'ﻣﺆﻃﺮ', 'ﺑﻨﺼﻮﺹ', 'ﻭﻣﻘﺘﻀﻴﺎﺕ', 'ﺷﺮﻋﻴﺔ', 'ﻭﺗﺸﺮﻳﻌﻴﺔ،', 'ﻳﺸﺘﺮﻁ', 'ﻋﺪﻡ', 'ﺗﺠﺎﻭﺯ', 'ﺛﻠﺚ', 'ﺍﻟﺤﻖ', 'ﺍﻟﻤﻮﺭﻭﺙ', 'ﺑﺈﺫﻥ', 'ﺍﻟﻮﺭﺛﺔ', 'ﻭﻟﻴﺲ', 'ﺍﻟﺘﺮﻛﺔ،', 'ﺑﺼﺮﻳﺢ', 'ﻣﻨﻄﻮﻕ', 'ﻧﺼﻮﺹ', 'ﺍﻟﺸﺮﻳﻌﺔ', 'ﺍﻹﺳﻼﻣﻴﺔ', 'ﻭﻟﻔﺖ', 'ﺍﻟﻤﺘﺤﺪﺙ', 'ﻋﻴﻨﻪ', 'ﺇﺷﻜﺎﻟﻴﺔ', 'ﻛﺒﻴﺮﺓ،', 'ﺗﺘﻌﻠﻖ،', 'ﺍﻟﻨﺎﺣﻴﺔ', 'ﺍﻟﺤﻘﻮﻗﻴﺔ،', 'ﺑﺈﺧﻀﺎﻉ', 'ﺍﻟﻤﺴﻠﻤﻴﻦ', 'ﻟﻘﻮﺍﻋﺪ', 'ﺍﻹﺭﺙ', 'ﺍﻹﺳﻼﻣﻴﺔ،', 'ﻣﺆﻛﺪﺍ', 'ﺃﻧﻪ', 'ﺗﻜﻦ', 'ﺇﺭﺍﺩﺗﻬﻢ', 'ﺍﻟﺘﺼﺮﻑ', 'ﺗﺮﻛﺘﻬﻢ؛', 'ﻓﻌﻠﻰ', 'ﺍﻟﻤﺸﺮﻉ', 'ﺍﻟﻤﻐﺮﺑﻲ', 'ﺇﻳﺠﺎﺩ', 'ﺣﻞ', 'ﻣﻨﺎﺳﺐ،', 'ﺧﻼﻝ', 'ﺇﻗﺮﺍﺭ', 'ﻣﺨﺮﺝ', 'ﺁﺧﺮ،', 'ﺑﻌﻴﺪ', 'ﻗﻮﺍﻋﺪ', 'ﺍﻹﺭﺙ', 'ﻭﺍﻟﻮﺻﻴﺔ', 'ﻭﻓﻖ', 'ﺍﻟﺸﺮﻳﻌﺔ', 'ﺍﻹﺳﻼﻣﻴﺔ،', 'ﻏﺮﺍﺭ', 'ﻣﺪﻭﻧﺔ', 'ﺍﻷﺣﻮﺍﻝ', 'ﺍﻟﺸﺨﺼﻴﺔ', 'ﺍﻟﻌﺒﺮﻳﺔ', 'ﺍﻟﺨﺎﺻﺔ', 'ﺑﺎﻟﻴﻬﻮﺩ', 'ﺍﻟﻤﻐﺎﺭﺑﺔ']]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "tokens = [all_posts[\"tokens\"][0]]\n",
        "print(\"tokens : \",tokens)\n",
        "\n",
        "# Train Word2Vec model (Skip-gram)\n",
        "model_skipgram = Word2Vec(sentences=tokens, vector_size=100, window=5, sg=1, min_count=1)\n",
        "\n",
        "# Train Word2Vec model (CBOW)\n",
        "model_cbow = Word2Vec(sentences=tokens, vector_size=100, window=5, sg=0, min_count=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "9feee3fb-2c94-422a-9ab9-9e5a1a52d15a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9feee3fb-2c94-422a-9ab9-9e5a1a52d15a",
        "outputId": "79eb9b0d-4725-4e29-f924-6d6874982430"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Similar words in skipgram model: [('ﺍﻟﻤﻘﺮﺭ', 0.29902079701423645), ('ﺍﻹﺳﻼﻣﻲ', 0.2990207076072693), ('ﻟﻜﻮﻧﻬﺎ', 0.2962135970592499), ('ﺍﻟﻮﺻﻴﺔ', 0.2756243050098419), ('ﺍﻟﺤﻴﺎﺓ،', 0.24749760329723358), ('ﻣﺴﺠﻼ', 0.24696336686611176), ('ﻋﻴﺸﻬﻢ', 0.24479064345359802), ('ﺃﻧﻪ', 0.23344415426254272), ('ﺍﻷﺣﻔﺎﺩ', 0.23062673211097717), ('ﻟﻠﻌﻘﺎﺋﺪ', 0.21811528503894806)]\n",
            "**************************************************\n",
            "Similar words in CBOW model: [('ﺍﻹﺳﻼﻣﻲ', 0.2946036159992218), ('ﺍﻟﻤﻘﺮﺭ', 0.29393765330314636), ('ﻟﻜﻮﻧﻬﺎ', 0.2917657196521759), ('ﺍﻟﻮﺻﻴﺔ', 0.2759097218513489), ('ﺍﻟﺤﻴﺎﺓ،', 0.2490367293357849), ('ﻣﺴﺠﻼ', 0.24520576000213623), ('ﻋﻴﺸﻬﻢ', 0.2443116158246994), ('ﺃﻧﻪ', 0.23288695514202118), ('ﺍﻷﺣﻔﺎﺩ', 0.2284097671508789), ('ﻟﻠﻌﻘﺎﺋﺪ', 0.21526853740215302)]\n"
          ]
        }
      ],
      "source": [
        "# Get the most similar words using skipgram model\n",
        "similar_words = model_skipgram.wv.most_similar(\"ﺍﻋﺘﺒﺮ\")\n",
        "print(\"Similar words in skipgram model:\", similar_words)\n",
        "\n",
        "print(\"*\"*50)\n",
        "\n",
        "# Get the most Find similar words using CBOW model\n",
        "similar_words_cbow = model_cbow.wv.most_similar(\"ﺍﻋﺘﺒﺮ\")\n",
        "print(\"Similar words in CBOW model:\", similar_words_cbow)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Y0dX0c0riE8C",
      "metadata": {
        "id": "Y0dX0c0riE8C"
      },
      "source": [
        "## 3. Apply Glove and FastText approaches on the same DataSet"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lqsp0C8H301G",
      "metadata": {
        "id": "lqsp0C8H301G"
      },
      "source": [
        "* example of usage :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "74eEf04ciEFy",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74eEf04ciEFy",
        "outputId": "9cc4409a-742d-4fde-efc6-03ef71e755b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word vector (FastText): [ 1.89042388e-04 -1.75465026e-03  4.86666337e-03 -6.49434514e-05\n",
            " -2.49946443e-03  1.10194110e-03 -3.16542527e-03 -6.47463952e-04\n",
            "  1.05347380e-03  1.73514965e-03 -2.82589323e-03 -2.35414860e-04\n",
            "  1.18078629e-03 -2.26751133e-03 -1.29877066e-03 -4.65275458e-04\n",
            " -3.11134121e-04 -1.10603869e-03 -1.89552002e-03 -2.33412185e-03\n",
            " -1.11964799e-03  2.43803114e-03 -3.08466115e-04 -1.44147605e-03\n",
            " -4.14242316e-03  1.18922198e-03 -1.49625342e-03  4.02119523e-03\n",
            " -4.93560219e-03  2.45301169e-03 -3.39337229e-03  1.10391724e-04\n",
            "  3.39285983e-03  3.01115419e-04 -1.57361035e-03  1.78972317e-03\n",
            " -1.28133141e-03  6.26029389e-04  1.45860983e-03  8.51550663e-04\n",
            "  2.52685696e-03  1.86977163e-03 -2.43563741e-03 -8.60533968e-04\n",
            " -4.53222397e-04  1.30850822e-04 -1.69015618e-03 -1.28772110e-03\n",
            "  2.58195703e-03 -2.22887262e-03 -3.97515047e-04 -2.76276842e-05\n",
            " -4.42006101e-04 -3.66209948e-04 -5.66424709e-03 -1.23990772e-04\n",
            "  7.32840155e-04  1.50242657e-03  1.85262959e-03  2.26473110e-03\n",
            "  1.38804282e-03 -1.32853328e-03 -4.13305417e-04  2.45495886e-03\n",
            "  3.05817137e-03  2.74912594e-03 -2.04217969e-03  2.27016374e-03\n",
            " -2.95168953e-03 -9.66600899e-04 -4.44000209e-04  2.60179187e-03\n",
            " -4.67995880e-04 -1.05647770e-04 -2.37910077e-04 -1.78460986e-03\n",
            " -5.19628120e-05 -4.65291645e-03 -2.38235807e-03 -5.75842394e-04\n",
            " -8.12121842e-04 -8.07073375e-04 -4.05465765e-03  3.59952194e-03\n",
            "  4.26721154e-03 -2.73191486e-03  1.72909349e-05 -5.59673004e-04\n",
            "  2.73835403e-03 -1.53826084e-03  1.58217072e-03 -5.25462488e-03\n",
            " -1.76962337e-03  1.82578922e-03 -1.64553302e-03 -1.02569298e-04\n",
            "  4.98657255e-03 -1.21641089e-03  2.32962170e-03  5.80017106e-04]\n",
            "Similar words (FastText): [('الشتاء', 0.17457064986228943), ('الطقس', 0.13070277869701385), ('بارداً', 0.011403929442167282), ('حاراً', -0.009513881988823414), ('في', -0.016105515882372856), ('و', -0.024582045152783394), ('الصيف', -0.025809271261096)]\n"
          ]
        }
      ],
      "source": [
        "from gensim.models import FastText\n",
        "\n",
        "# Example Arabic tokens\n",
        "arabic_tokens = [[\"كان\", \"الطقس\", \"حاراً\", \"في\", \"الصيف\", \"و\", \"بارداً\", \"في\", \"الشتاء\"]]\n",
        "\n",
        "# Train FastText model\n",
        "model_fasttext = FastText(sentences=arabic_tokens, vector_size=100, window=5, min_count=1)\n",
        "\n",
        "# Get vector representation of a word\n",
        "word_vector_fasttext = model_fasttext.wv.get_vector(\"كان\")\n",
        "\n",
        "# Find similar words\n",
        "similar_words_fasttext = model_fasttext.wv.most_similar(\"كان\")\n",
        "\n",
        "print(\"Word vector (FastText):\", word_vector_fasttext)\n",
        "print(\"Similar words (FastText):\", similar_words_fasttext)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "buAE9p5z35dI",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "buAE9p5z35dI",
        "outputId": "286d86a5-f0d8-484c-f7a1-898f754ab8a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting glove\n",
            "  Using cached glove-1.0.2.tar.gz (44 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from glove) (1.25.2)\n",
            "Building wheels for collected packages: glove\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for glove (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for glove\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for glove\n",
            "Failed to build glove\n",
            "\u001b[31mERROR: Could not build wheels for glove, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install glove"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "C_OARCIj3Jbk",
      "metadata": {
        "id": "C_OARCIj3Jbk"
      },
      "outputs": [],
      "source": [
        "from glove import Corpus, Glove\n",
        "\n",
        "# Train GloVe model\n",
        "corpus = Corpus()\n",
        "corpus.fit(arabic_tokens, window=5)\n",
        "glove = Glove(no_components=100, learning_rate=0.05)\n",
        "glove.fit(corpus.matrix, epochs=30, no_threads=4, verbose=True)\n",
        "\n",
        "# Get vector representation of a word\n",
        "word_vector_glove = glove.word_vectors[glove.dictionary[\"كان\"]]\n",
        "\n",
        "# Find similar words\n",
        "similar_words_glove = glove.most_similar(\"كان\")\n",
        "\n",
        "print(\"Word vector (GloVe):\", word_vector_glove)\n",
        "print(\"Similar words (GloVe):\", similar_words_glove)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_1aDzCzR4BS-",
      "metadata": {
        "id": "_1aDzCzR4BS-"
      },
      "source": [
        "* Apply on real data :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "XZ2LnLp9iEC-",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZ2LnLp9iEC-",
        "outputId": "99f3ae59-2aff-474e-c6f3-01e1ed262cef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tokens :  ['ﺿﻤﻦ', 'ﺍﻟﺘﻘﺮﻳﺮ', 'ﺗﺴﻠﻤﻪ', 'ﻋﺰﻳﺰ', 'ﺃﺧﻨﻮﺵ،', 'ﺭﺋﻴﺲ', 'ﺍﻟﺤﻜﻮﻣﺔ،', 'ﻧﻬﺎﻳﺔ', 'ﺷﻬﺮ', 'ﺍﻟﻤﺎﺿﻲ،', 'ﺍﻟﻬﻴﺌﺔ', 'ﺍﻟﻤﻜﻠﻔﺔ', 'ﺑﻤﺮﺍﺟﻌﺔ', 'ﻣﺪﻭﻧﺔ', 'ﺍﻷﺳﺮﺓ،', 'ﺍﻟﻤﻘﺮﺭ', 'ﺭﻓﻌﻪ', 'ﺍﻟﻤﻠﻚ', 'ﻣﺤﻤﺪ', 'ﺍﻟﺴﺎﺩﺱ،', 'ﻻﻓﺘﺎ', 'ﻭﺟﻮﺩ', 'ﻣﻘﺘﺮﺡ', 'ﺗﻄﺮﻕ', 'ﺗﻘﺴﻴﻢ', 'ﺍﻟﺘﺮﻛﺔ', 'ﺧﻼﻝ', 'ﺗﺨﻮﻳﻞ', 'ﺻﺎﺣﺐ', 'ﺍﻟﻤﺎﻝ', 'ﻭﺍﻷﺻﻮﻝ', 'ﺳﻠﻄﺔ', 'ﺍﺧﺘﻴﺎﺭ', 'ﺍﻟﻨﻈﺎﻡ', 'ﺍﻟﻤﻄﺒﻖ،', 'ﺍﻟﻤﻴﺮﺍﺙ', 'ﺍﻟﻮﺻﻴﺔ،', 'ﺗﻮﺳﻴﻊ', 'ﻧﻄﺎﻕ', 'ﺍﻷﺧﻴﺮﺓ', 'ﻟﺘﺸﻤﻞ', 'ﺍﻷﺣﻔﺎﺩ', 'ﻭﺭﻓﻊ', 'ﺍﻟﻘﻴﻮﺩ', 'ﺍﻟﻤﻔﺮﻭﺿﺔ', 'ﻋﻠﻴﻬﺎ', 'ﻭﻋﻠﻰ', 'ﺍﻟﺮﻏﻢ', 'ﺍﻟﻤﻘﺘﺮﺡ،', 'ﻋﺪﻩ', 'ﻛﺜﻴﺮﻭﻥ', 'ﺗﺴﺮﻳﺒﺎ،', 'ﻋﻤﻮﻣﺎ', 'ﻭﺛﻴﻘﺔ', 'ﺭﺍﺋﺠﺔ', 'ﻳﺘﺼﻞ', 'ﺑﺘﻮﺻﻴﺎﺕ', 'ﺭﺳﻤﻴﺔ', 'ﻣﺮﻓﻮﻋﺔ', 'ﺍﻟﻤﻠﻚ', 'ﻣﺤﻤﺪ', 'ﺍﻟﺴﺎﺩﺱ؛', 'ﺟﻬﺎﺕ', 'ﻛﺜﻴﺮﺓ', 'ﺍﻟﺘﻘﻄﺘﻪ،', 'ﺧﺼﻮﺻﺎ', 'ﺍﻷﻗﻠﻴﺎﺕ', 'ﺍﻟﺪﻳﻨﻴﺔ', 'ﺑﺎﻟﻤﻐﺮﺏ،', 'ﻭﻧﻈﺮﺕ', 'ﺇﻟﻴﻪ', 'ﺑـﺘﻘﺪﻳﺮ', 'ﻛـﺤﻞ', 'ﺑﺮﺍﻏﻤﺎﺗﻲ', 'ﻳﺴﺎﻋﺪ', 'ﺗﻘﺴﻴﻢ', 'ﺍﻟﺘﺮﻛﺎﺕ', 'ﻭﻓﻖ', 'ﻣﻨﻄﻖ', 'ﺍﻟﻮﺻﻴﺔ،', 'ﻭﺗﻤﻜﻴﻦ', 'ﺍﻟﻤﻐﺎﺭﺑﺔ', 'ﺍﺧﺘﻴﺎﺭ', 'ﻳﻨﺎﺳﺐ', 'ﺷﻜﻞ', 'ﻋﻴﺸﻬﻢ', 'ﺍﻟﺪﻳﻦ', 'ﻭﺍﻟﻘﺎﻧﻮﻥ', 'ﺳﻌﻴﺪ', 'ﻧﺎﺷﻴﺪ،', 'ﺣﻘﻮﻗﻲ', 'ﻭﺑﺎﺣﺚ', 'ﺍﻟﻔﻜﺮ', 'ﺍﻟﺪﻳﻨﻲ،', 'ﺍﻋﺘﺒﺮ', 'ﻣﻘﺘﺮﺡ', 'ﺗﻀﻤﻴﻦ', 'ﺍﻟﻮﺻﻴﺔ', 'ﺿﻤﻦ', 'ﺍﻻﺧﺘﻴﺎﺭﺍﺕ']\n",
            "Similar words (FastText): [('ﻭﺍﻋﺘﺒﺮ', 0.4191701114177704), ('ﺍﻟﺤﻜﻮﻣﺔ،', 0.27876144647598267), ('ﻣﻌﻤﻮﻝ', 0.2516515851020813), ('ﻳﺴﺎﻋﺪ', 0.2405984103679657), ('ﺧﺎﻃﺊ', 0.23860421776771545), ('ﺗﺆﻃﺮﻫﺎ', 0.23427118360996246), ('ﺃﻣﻴﺮ', 0.23051756620407104), ('ﺗﻮﻗﻴﻌﺎ', 0.20396675169467926), ('ﺗﺒﺮﻋﻲ', 0.20203030109405518), ('ﺗﻌﻨﻲ', 0.19796393811702728)]\n"
          ]
        }
      ],
      "source": [
        "tokens = [all_posts[\"tokens\"][0]]\n",
        "print(\"tokens : \",tokens[0][:100])\n",
        "\n",
        "# Train FastText model\n",
        "model_fasttext = FastText(sentences=tokens, vector_size=100, window=5, min_count=1)\n",
        "\n",
        "# Find similar words\n",
        "similar_words_fasttext = model_fasttext.wv.most_similar(\"ﺍﻋﺘﺒﺮ\")\n",
        "\n",
        "print(\"Similar words (FastText):\", similar_words_fasttext)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YGqhByXc4xS6",
      "metadata": {
        "id": "YGqhByXc4xS6"
      },
      "source": [
        "### 4. Plot all the encoded / vectorized vectors by using Tsne Algorithm, evaluate those approaches and give a general conclusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oAGJGOzD47il",
      "metadata": {
        "id": "oAGJGOzD47il"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
